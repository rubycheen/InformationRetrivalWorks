{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IR Programming Assignment2\n",
    "# 資管三 B06406009 陳姵如\n",
    "\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import sys\n",
    "import csv\n",
    "import math\n",
    "import string\n",
    "import numpy\n",
    "import pandas as pd\n",
    "from nltk import PorterStemmer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the class index\n",
    "\n",
    "index = []\n",
    "\n",
    "file = open('13classes.txt', 'r')\n",
    "\n",
    "for i in range (13):\n",
    "    index.append((file.readline()).split())\n",
    "    del index[i][0]\n",
    "\n",
    "file.close()\n",
    "\n",
    "for i in range (13):\n",
    "    for j in range (15):\n",
    "        index[i][j] = str(index[i][j]) + '.txt'\n",
    "\n",
    "# print(len(index[12]), index[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poter's Algorithm\n",
    "\n",
    "ClassifiedDocument = []\n",
    "\n",
    "class PorterStemmer:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.b = \"\"  # buffer for word to be stemmed\n",
    "        self.k = 0\n",
    "        self.k0 = 0\n",
    "        self.j = 0   # j is a general offset into the string\n",
    "\n",
    "    def cons(self, i):\n",
    "        if self.b[i] == 'a' or self.b[i] == 'e' or self.b[i] == 'i' or self.b[i] == 'o' or self.b[i] == 'u':\n",
    "            return 0\n",
    "        if self.b[i] == 'y':\n",
    "            if i == self.k0:\n",
    "                return 1\n",
    "            else:\n",
    "                return (not self.cons(i - 1))\n",
    "        return 1\n",
    "\n",
    "    def m(self):\n",
    "        n = 0\n",
    "        i = self.k0\n",
    "        while 1:\n",
    "            if i > self.j:\n",
    "                return n\n",
    "            if not self.cons(i):\n",
    "                break\n",
    "            i = i + 1\n",
    "        i = i + 1\n",
    "        while 1:\n",
    "            while 1:\n",
    "                if i > self.j:\n",
    "                    return n\n",
    "                if self.cons(i):\n",
    "                    break\n",
    "                i = i + 1\n",
    "            i = i + 1\n",
    "            n = n + 1\n",
    "            while 1:\n",
    "                if i > self.j:\n",
    "                    return n\n",
    "                if not self.cons(i):\n",
    "                    break\n",
    "                i = i + 1\n",
    "            i = i + 1\n",
    "\n",
    "    def vowelinstem(self):\n",
    "        for i in range(self.k0, self.j + 1):\n",
    "            if not self.cons(i):\n",
    "                return 1\n",
    "        return 0\n",
    "\n",
    "    def doublec(self, j):\n",
    "        if j < (self.k0 + 1):\n",
    "            return 0\n",
    "        if (self.b[j] != self.b[j-1]):\n",
    "            return 0\n",
    "        return self.cons(j)\n",
    "\n",
    "    def cvc(self, i):\n",
    "        if i < (self.k0 + 2) or not self.cons(i) or self.cons(i-1) or not self.cons(i-2):\n",
    "            return 0\n",
    "        ch = self.b[i]\n",
    "        if ch == 'w' or ch == 'x' or ch == 'y':\n",
    "            return 0\n",
    "        return 1\n",
    "\n",
    "    def ends(self, s):\n",
    "        length = len(s)\n",
    "        if s[length - 1] != self.b[self.k]: # tiny speed-up\n",
    "            return 0\n",
    "        if length > (self.k - self.k0 + 1):\n",
    "            return 0\n",
    "        if self.b[self.k-length+1:self.k+1] != s:\n",
    "            return 0\n",
    "        self.j = self.k - length\n",
    "        return 1\n",
    "\n",
    "    def setto(self, s):\n",
    "        length = len(s)\n",
    "        self.b = self.b[:self.j+1] + s + self.b[self.j+length+1:]\n",
    "        self.k = self.j + length\n",
    "\n",
    "    def r(self, s):\n",
    "        if self.m() > 0:\n",
    "            self.setto(s)\n",
    "\n",
    "    def step1ab(self):\n",
    "        \"\"\"step1ab() gets rid of plurals and -ed or -ing. e.g.\n",
    "\n",
    "           caresses  ->  caress\n",
    "           ponies    ->  poni\n",
    "           ties      ->  ti\n",
    "           caress    ->  caress\n",
    "           cats      ->  cat\n",
    "\n",
    "           feed      ->  feed\n",
    "           agreed    ->  agree\n",
    "           disabled  ->  disable\n",
    "\n",
    "           matting   ->  mat\n",
    "           mating    ->  mate\n",
    "           meeting   ->  meet\n",
    "           milling   ->  mill\n",
    "           messing   ->  mess\n",
    "\n",
    "           meetings  ->  meet\n",
    "        \"\"\"\n",
    "        if self.b[self.k] == 's':\n",
    "            if self.ends(\"sses\"):\n",
    "                self.k = self.k - 2\n",
    "            elif self.ends(\"ies\"):\n",
    "                self.setto(\"i\")\n",
    "            elif self.b[self.k - 1] != 's':\n",
    "                self.k = self.k - 1\n",
    "        if self.ends(\"eed\"):\n",
    "            if self.m() > 0:\n",
    "                self.k = self.k - 1\n",
    "        elif (self.ends(\"ed\") or self.ends(\"ing\")) and self.vowelinstem():\n",
    "            self.k = self.j\n",
    "            if self.ends(\"at\"):   self.setto(\"ate\")\n",
    "            elif self.ends(\"bl\"): self.setto(\"ble\")\n",
    "            elif self.ends(\"iz\"): self.setto(\"ize\")\n",
    "            elif self.doublec(self.k):\n",
    "                self.k = self.k - 1\n",
    "                ch = self.b[self.k]\n",
    "                if ch == 'l' or ch == 's' or ch == 'z':\n",
    "                    self.k = self.k + 1\n",
    "            elif (self.m() == 1 and self.cvc(self.k)):\n",
    "                self.setto(\"e\")\n",
    "\n",
    "    def step1c(self):\n",
    "        \"\"\"step1c() turns terminal y to i when there is another vowel in the stem.\"\"\"\n",
    "        if (self.ends(\"y\") and self.vowelinstem()):\n",
    "            self.b = self.b[:self.k] + 'i' + self.b[self.k+1:]\n",
    "\n",
    "    def step2(self):\n",
    "        \"\"\"step2() maps double suffices to single ones.\n",
    "        so -ization ( = -ize plus -ation) maps to -ize etc. note that the\n",
    "        string before the suffix must give m() > 0.\n",
    "        \"\"\"\n",
    "        if self.b[self.k - 1] == 'a':\n",
    "            if self.ends(\"ational\"):   self.r(\"ate\")\n",
    "            elif self.ends(\"tional\"):  self.r(\"tion\")\n",
    "        elif self.b[self.k - 1] == 'c':\n",
    "            if self.ends(\"enci\"):      self.r(\"ence\")\n",
    "            elif self.ends(\"anci\"):    self.r(\"ance\")\n",
    "        elif self.b[self.k - 1] == 'e':\n",
    "            if self.ends(\"izer\"):      self.r(\"ize\")\n",
    "        elif self.b[self.k - 1] == 'l':\n",
    "            if self.ends(\"bli\"):       self.r(\"ble\") # --DEPARTURE--\n",
    "            # To match the published algorithm, replace this phrase with\n",
    "            #   if self.ends(\"abli\"):      self.r(\"able\")\n",
    "            elif self.ends(\"alli\"):    self.r(\"al\")\n",
    "            elif self.ends(\"entli\"):   self.r(\"ent\")\n",
    "            elif self.ends(\"eli\"):     self.r(\"e\")\n",
    "            elif self.ends(\"ousli\"):   self.r(\"ous\")\n",
    "        elif self.b[self.k - 1] == 'o':\n",
    "            if self.ends(\"ization\"):   self.r(\"ize\")\n",
    "            elif self.ends(\"ation\"):   self.r(\"ate\")\n",
    "            elif self.ends(\"ator\"):    self.r(\"ate\")\n",
    "        elif self.b[self.k - 1] == 's':\n",
    "            if self.ends(\"alism\"):     self.r(\"al\")\n",
    "            elif self.ends(\"iveness\"): self.r(\"ive\")\n",
    "            elif self.ends(\"fulness\"): self.r(\"ful\")\n",
    "            elif self.ends(\"ousness\"): self.r(\"ous\")\n",
    "        elif self.b[self.k - 1] == 't':\n",
    "            if self.ends(\"aliti\"):     self.r(\"al\")\n",
    "            elif self.ends(\"iviti\"):   self.r(\"ive\")\n",
    "            elif self.ends(\"biliti\"):  self.r(\"ble\")\n",
    "        elif self.b[self.k - 1] == 'g': # --DEPARTURE--\n",
    "            if self.ends(\"logi\"):      self.r(\"log\")\n",
    "        # To match the published algorithm, delete this phrase\n",
    "\n",
    "    def step3(self):\n",
    "        \"\"\"step3() dels with -ic-, -full, -ness etc. similar strategy to step2.\"\"\"\n",
    "        if self.b[self.k] == 'e':\n",
    "            if self.ends(\"icate\"):     self.r(\"ic\")\n",
    "            elif self.ends(\"ative\"):   self.r(\"\")\n",
    "            elif self.ends(\"alize\"):   self.r(\"al\")\n",
    "        elif self.b[self.k] == 'i':\n",
    "            if self.ends(\"iciti\"):     self.r(\"ic\")\n",
    "        elif self.b[self.k] == 'l':\n",
    "            if self.ends(\"ical\"):      self.r(\"ic\")\n",
    "            elif self.ends(\"ful\"):     self.r(\"\")\n",
    "        elif self.b[self.k] == 's':\n",
    "            if self.ends(\"ness\"):      self.r(\"\")\n",
    "\n",
    "    def step4(self):\n",
    "        \"\"\"step4() takes off -ant, -ence etc., in context <c>vcvc<v>.\"\"\"\n",
    "        if self.b[self.k - 1] == 'a':\n",
    "            if self.ends(\"al\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 'c':\n",
    "            if self.ends(\"ance\"): pass\n",
    "            elif self.ends(\"ence\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 'e':\n",
    "            if self.ends(\"er\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 'i':\n",
    "            if self.ends(\"ic\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 'l':\n",
    "            if self.ends(\"able\"): pass\n",
    "            elif self.ends(\"ible\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 'n':\n",
    "            if self.ends(\"ant\"): pass\n",
    "            elif self.ends(\"ement\"): pass\n",
    "            elif self.ends(\"ment\"): pass\n",
    "            elif self.ends(\"ent\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 'o':\n",
    "            if self.ends(\"ion\") and (self.b[self.j] == 's' or self.b[self.j] == 't'): pass\n",
    "            elif self.ends(\"ou\"): pass\n",
    "            # takes care of -ous\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 's':\n",
    "            if self.ends(\"ism\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 't':\n",
    "            if self.ends(\"ate\"): pass\n",
    "            elif self.ends(\"iti\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 'u':\n",
    "            if self.ends(\"ous\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 'v':\n",
    "            if self.ends(\"ive\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 'z':\n",
    "            if self.ends(\"ize\"): pass\n",
    "            else: return\n",
    "        else:\n",
    "            return\n",
    "        if self.m() > 1:\n",
    "            self.k = self.j\n",
    "\n",
    "    def step5(self):\n",
    "        \"\"\"step5() removes a final -e if m() > 1, and changes -ll to -l if\n",
    "        m() > 1.\n",
    "        \"\"\"\n",
    "        self.j = self.k\n",
    "        if self.b[self.k] == 'e':\n",
    "            a = self.m()\n",
    "            if a > 1 or (a == 1 and not self.cvc(self.k-1)):\n",
    "                self.k = self.k - 1\n",
    "        if self.b[self.k] == 'l' and self.doublec(self.k) and self.m() > 1:\n",
    "            self.k = self.k -1\n",
    "\n",
    "    def stem(self, p, i, j):\n",
    "        # copy the parameters into statics\n",
    "        self.b = p\n",
    "        self.k = j\n",
    "        self.k0 = i\n",
    "        if self.k <= self.k0 + 1:\n",
    "            return self.b # --DEPARTURE--\n",
    "\n",
    "        # With this line, strings of length 1 or 2 don't go through the\n",
    "        # stemming process, although no mention is made of this in the\n",
    "        # published algorithm. Remove the line to match the published\n",
    "        # algorithm.\n",
    "\n",
    "        self.step1ab()\n",
    "        self.step1c()\n",
    "        self.step2()\n",
    "        self.step3()\n",
    "        self.step4()\n",
    "        self.step5()\n",
    "        return self.b[self.k0:self.k+1]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    p = PorterStemmer()\n",
    "    if len(sys.argv) > 1:\n",
    "        for f in sys.argv[1:]:\n",
    "            for i in range (13): \n",
    "                tmp = []\n",
    "                for j in range (15):\n",
    "                    infile = (open(index[i][j], 'r'))\n",
    "                    tmp.append(infile)\n",
    "                    output = ''\n",
    "                    word = ''\n",
    "                    if tmp[j] == '':\n",
    "                        break\n",
    "                    for c in tmp[j]:\n",
    "                        if c.isalpha():\n",
    "                            word += c.lower()\n",
    "                        else:\n",
    "                            if word:\n",
    "                                output += p.stem(word, 0,len(word)-1)\n",
    "                                word = ''\n",
    "                            output += c.lower()\n",
    "                            tmp[j] = output \n",
    "                ClassifiedDocument.append(tmp)\n",
    "            infile.close()\n",
    "                \n",
    "# print(ClassifiedDocument[12][14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "\n",
    "TokenClassifiedDocument = [] #有重複\n",
    "\n",
    "for i in range (13): \n",
    "    tmp = []\n",
    "    for j in range (15):\n",
    "        tmp.append(word_tokenize(ClassifiedDocument[i][j]))\n",
    "    TokenClassifiedDocument.append(tmp)\n",
    "    \n",
    "# print(len(TokenClassifiedDocument[12][14]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopword removal\n",
    "# Redundancy removal\n",
    "\n",
    "TokenClassified = []\n",
    "\n",
    "redundency = ['!','.','?','$',\"'s\",\"''\",',',\"n't\",\"''\",\"'ve\",'would']\n",
    "stop_words = set(stopwords.words('english') + redundency)\n",
    "# print(stop_words)\n",
    "for i in range (13): \n",
    "    tmp2 = []\n",
    "    for j in range (15):\n",
    "        tmp = []\n",
    "        for w in range(len(TokenClassifiedDocument[i][j])): \n",
    "            if (TokenClassifiedDocument[i][j][w] not in stop_words) and (TokenClassifiedDocument[i][j][w].isalpha() == True):\n",
    "                tmp.append(TokenClassifiedDocument[i][j][w])\n",
    "        tmp2.append(tmp)\n",
    "    TokenClassified.append(tmp2)\n",
    "#     print(TokenClassified[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort\n",
    "for i in range (13): \n",
    "    for j in range (15):    \n",
    "        TokenClassified[i][j].sort()\n",
    "# print(TokenClassified[12][14])\n",
    "\n",
    "# Establish my classfied Documents\n",
    "\n",
    "TermClassifiedDocument = [] #每個文檔去掉重複\n",
    "ClassifiedDictionary = [] #13類的共用字典\n",
    "\n",
    "for i in range (13): \n",
    "    tmp2 = []\n",
    "    for j in range (15): \n",
    "        tmp = []\n",
    "        for k in range(len(TokenClassified[i][j])):\n",
    "            if TokenClassified[i][j][k] not in tmp:\n",
    "                tmp.append(TokenClassified[i][j][k])\n",
    "        tmp.sort()\n",
    "        tmp2.append(tmp)\n",
    "    TermClassifiedDocument.append(tmp2)\n",
    "#     print(TermClassifiedDocument[i][j])\n",
    "\n",
    "\n",
    "# Establish my classfied dictionary\n",
    "\n",
    "for i in range (13): \n",
    "    for j in range (15):    \n",
    "        for k in range(len(TermClassifiedDocument[i][j])):\n",
    "            if TermClassifiedDocument[i][j][k] not in ClassifiedDictionary:\n",
    "                ClassifiedDictionary.append(TermClassifiedDocument[i][j][k])\n",
    "                \n",
    "ClassifiedDictionary.sort()    \n",
    "\n",
    "# print(ClassifiedDictionary)\n",
    "# print(len(ClassifiedDictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Count the OntopicCnt and OfftopicCnt\n",
    "\n",
    "OntopicCnt = []\n",
    "OfftopicCnt = []\n",
    "\n",
    "for i in range (len(ClassifiedDictionary)):\n",
    "    tmp = []\n",
    "    for j in range(13):\n",
    "        cnt = 0\n",
    "        for k in range(15):\n",
    "            if ClassifiedDictionary[i] in TermClassifiedDocument[j][k]:\n",
    "                cnt = cnt+1\n",
    "        tmp.append(cnt)\n",
    "    OntopicCnt.append(tmp)\n",
    "\n",
    "print(OntopicCnt[len(ClassifiedDictionary)-1])\n",
    "    \n",
    "\n",
    "for i in range(len(ClassifiedDictionary)):\n",
    "    tmp = []\n",
    "    for block in range (13): # term 現在所在的類\n",
    "        cnt = 0\n",
    "        for j in range(13):\n",
    "            if j != block: # 跳過現在所在的類別\n",
    "                for k in range(15):\n",
    "                    if ClassifiedDictionary[i] in TermClassifiedDocument[j][k]:\n",
    "                        cnt = cnt+1\n",
    "        tmp.append(cnt)\n",
    "    OfftopicCnt.append(tmp) \n",
    "\n",
    "print(OfftopicCnt[len(ClassifiedDictionary)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the value of likelihood\n",
    "\n",
    "LikelihoodRatio = []\n",
    "pt = []\n",
    "p1 = []\n",
    "p2 = []\n",
    "\n",
    "for i in range(len(ClassifiedDictionary)):\n",
    "    tmppt = []\n",
    "    tmpp1 = []\n",
    "    tmpp2 = []\n",
    "    for j in range(13):\n",
    "        tmppt.append((OntopicCnt[i][j] + (180-OfftopicCnt[i][j]) + 1) / (195)) #弄懂加多少\n",
    "        tmpp1.append((OntopicCnt[i][j] + 1) / (15))\n",
    "        tmpp2.append((OfftopicCnt[i][j] + 1) / (180))\n",
    "    pt.append(tmppt)\n",
    "    p1.append(tmpp1)\n",
    "    p2.append(tmpp2)\n",
    "\n",
    "# print(pt[0])\n",
    "# print((p1[0]))\n",
    "# print((p2[0]))\n",
    "\n",
    "L1 = []\n",
    "L2 = []\n",
    "\n",
    "\n",
    "for i in range(len(ClassifiedDictionary)):\n",
    "    tmpL1 = []\n",
    "    tmpL2 = []\n",
    "    for j in range(13):\n",
    "        tmpL1.append((pt[i][j]**(OntopicCnt[i][j]+(180-OfftopicCnt[i][j])) * (1-pt[i][j]))**((15-OntopicCnt[i][j])+OfftopicCnt[i][j]))\n",
    "        tmpL2.append((p1[i][j]**OntopicCnt[i][j] * (1-p1[i][j])**(15-OntopicCnt[i][j]) * p2[i][j]**(180-OfftopicCnt[i][j]) * (1-p2[i][j])**OfftopicCnt[i][j]))\n",
    "    L1.append(tmpL1)\n",
    "    L2.append(tmpL2)\n",
    "\n",
    "# print(L1[0])\n",
    "\n",
    "# from scipy.stats.distributions import chi2\n",
    "def likelihood_ratio(L1, L2):\n",
    "    return(-2*(L1-L2))\n",
    "\n",
    "for j in range(13):\n",
    "    tmpLH = []\n",
    "    for i in range(len(ClassifiedDictionary)):\n",
    "        tmpLH.append(likelihood_ratio(L1[i][j],L2[i][j]))\n",
    "    LikelihoodRatio.append(tmpLH)\n",
    "\n",
    "# print(len(LikelihoodRatio[12]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 250 ['kursk', 'divers', 'barents', 'nuclear', 'norwegian', 'sub', 'recover', 'sank', 'sunken', 'collision', 'exercises', 'diving', 'seamen', 'accident', 'august', 'compartments', 'holes', 'hulls', 'inner', 'malfunction', 'norway', 'operation', 'bottom', 'compartment', 'explosions', 'floor', 'hull', 'internal', 'navrotsky', 'ocean', 'pipes', 'recovering', 'recovery', 'retrieve', 'serga', 'submarines', 'subsidiary', 'suits', 'vadim', 'vessel', 'waters', 'winds', 'actual', 'bell', 'capabilities', 'causes', 'coating', 'complicate', 'curtail', 'cutting', 'determined', 'gale', 'halliburton', 'ilya', 'klebanov', 'layer', 'misfire', 'outer', 'petersburg', 'reactors', 'rear', 'removing', 'rubin', 'salvage', 'shallow', 'sliced', 'sounds', 'space', 'steadily', 'successfully', 'surface', 'thicker', 'unless', 'accordance', 'admiral', 'aggression', 'aim', 'akishio', 'alexe', 'arctic', 'armaments', 'armory', 'avail', 'awkward', 'barent', 'bearded', 'bergen', 'biltera', 'birger', 'bled', 'britain', 'budnev', 'capsules', 'captain', 'catastrophic', 'changes', 'chechnya', 'cheney', 'chile', 'choi', 'closable', 'colliding', 'complicating', 'conclude', 'conqueror', 'contract', 'controversies', 'corrosive', 'creating', 'crewmembers', 'crewmen', 'cynical', 'dangers', 'depending', 'depends', 'deputy', 'designed', 'detect', 'deteriorating', 'devices', 'dick', 'disabled', 'dismantling', 'distress', 'downed', 'drastic', 'dygalo', 'emergencies', 'endanger', 'enlarged', 'enlarging', 'evanell', 'extricate', 'fifteen', 'findings', 'firstly', 'forgotten', 'formally', 'fudged', 'gladkovich', 'gop', 'grabbed', 'grief', 'gulls', 'haliberton', 'hammerfest', 'haraldseid', 'hatches', 'heaps', 'helena', 'improving', 'incense', 'inspected', 'inspecting', 'intentions', 'invest', 'involves', 'itogi', 'jagged', 'jds', 'laborious', 'leftover', 'leidig', 'leslie', 'lied', 'lies', 'limited', 'limiting', 'littoral', 'maneuver', 'meaning', 'mechanism', 'misfired', 'missile', 'monitored', 'monitoring', 'moo', 'muffle', 'murmansk', 'naval', 'navies', 'neutralized', 'nominee', 'nonstop', 'norwegians', 'obstructions', 'openings', 'organize', 'pedro', 'penetration', 'pierced', 'possibilities', 'practical', 'precarious', 'preparations', 'preparatory', 'pried', 'priests', 'psychological', 'pursues', 'radiation', 'raising', 'referred', 'regalia', 'remembrance', 'removal', 'renounced', 'respected', 'resume', 'retrieval', 'retrieved', 'retrieving', 'roks', 'roll', 'rss', 'scandinavian', 'scrap', 'sea', 'seabed', 'seas', 'sebek', 'secondly', 'seconds', 'sections', 'shallower', 'simulating', 'sinking', 'spassky', 'stands', 'stolt', 'stored', 'storms', 'sturdier', 'sturdy', 'submariners', 'submerging', 'submersibles', 'subs', 'sum', 'sunk', 'suspend', 'suspension', 'tananger', 'technicians', 'theories', 'thirdly', 'tomb', 'torches', 'transfer', 'understanding']\n",
      "1 250 ['milosevic', 'slobodan', 'belgrade', 'kostunica', 'vojislav', 'yugoslavia', 'serbia', 'yugoslav', 'coal', 'concede', 'majority', 'serbian', 'disobedience', 'jobs', 'kosovo', 'strike', 'mediate', 'outright', 'runoff', 'serb', 'serbs', 'backing', 'blockades', 'chancellor', 'conflicts', 'confrontation', 'conversation', 'electoral', 'essay', 'gerhard', 'guaranteed', 'miners', 'nato', 'newsweek', 'recognized', 'refusing', 'relinquish', 'schroeder', 'shown', 'strikes', 'tossed', 'abiding', 'agreed', 'ally', 'alphabets', 'balkan', 'behavior', 'berlin', 'blocked', 'borne', 'bosnian', 'brunt', 'bullied', 'butchered', 'cemented', 'chased', 'claim', 'conceded', 'contempt', 'countrymen', 'coverage', 'critical', 'croat', 'croatia', 'croatian', 'dating', 'defeat', 'delusion', 'demagogue', 'demands', 'demonize', 'dictator', 'diplomats', 'dishonoring', 'diversion', 'djindjic', 'doctoring', 'dress', 'eager', 'egotistical', 'electric', 'europe', 'fascists', 'fiction', 'figured', 'forcing', 'franjo', 'functionary', 'fundamentalists', 'german', 'handy', 'ignominious', 'ignoring', 'inflame', 'intersections', 'inventing', 'itching', 'ivanov', 'joined', 'kissed', 'kolubara', 'kosovar', 'laying', 'legitimacy', 'linking', 'logic', 'loyalists', 'manufacture', 'mediation', 'mines', 'minorities', 'mistreat', 'mistreatment', 'murderous', 'mutilated', 'nationalist', 'nationalistic', 'nazis', 'newscasters', 'offer', 'options', 'organized', 'orthodox', 'ottoman', 'paranoia', 'pariah', 'photograph', 'plugged', 'podgorica', 'populist', 'posturing', 'poverty', 'presided', 'protecting', 'protests', 'religions', 'remembering', 'republics', 'resentments', 'resistance', 'responded', 'roads', 'sarajevo', 'shared', 'showcased', 'siege', 'somehow', 'splintering', 'status', 'steal', 'strategy', 'stripped', 'suppressed', 'survival', 'survive', 'sympathy', 'taxi', 'terribly', 'terrorize', 'tito', 'traffic', 'trappings', 'troubles', 'tudjman', 'tyrant', 'victimization', 'votes', 'wait', 'witless', 'yugoslavs', 'zoran', 'ab', 'ability', 'absolute', 'abundant', 'academy', 'accuse', 'achieve', 'acknowledging', 'activities', 'adds', 'admit', 'affirm', 'ahead', 'aimed', 'aj', 'albanians', 'albert', 'aleksa', 'alibi', 'allied', 'amazed', 'antasiaovich', 'anxiety', 'anxiously', 'apparatus', 'appointed', 'astonished', 'astronomical', 'aura', 'balanced', 'balkans', 'banging', 'banners', 'beta', 'blackmailing', 'blockade', 'blockaded', 'bowing', 'brandished', 'breakup', 'breme', 'bridge', 'bridges', 'briefed', 'brutality', 'c', 'cacak', 'cafes', 'carrot', 'casetch', 'certification', 'cheating', 'chetsov', 'chizhov', 'chizov', 'circle', 'coalmine', 'collectors', 'colobora', 'columnist', 'commented', 'communism', 'commuters', 'complaining', 'conflict', 'confront', 'congratulated', 'cons', 'constantinescu', 'controlled', 'controls', 'convincing', 'convoy', 'cornette', 'counter', 'courageously', 'credits', 'crimes', 'culminate']\n",
      "2 250 ['el', 'guatemala', 'salvadoran', 'central', 'colinas', 'cross', 'las', 'bare', 'mexico', 'miguel', 'rev', 'santa', 'ana', 'arturo', 'buried', 'calvario', 'cellular', 'centered', 'church', 'dirt', 'frantically', 'honduras', 'knocked', 'landslide', 'magana', 'mass', 'sosonati', 'sticks', 'worshipping', 'alarms', 'brother', 'collapse', 'concrete', 'cracked', 'denver', 'dig', 'electricity', 'flores', 'girl', 'honduran', 'jaime', 'jalpataua', 'neighborhood', 'rescuers', 'ruins', 'sound', 'spotty', 'structurally', 'suchitepequez', 'temporarily', 'wandered', 'woman', 'areas', 'beside', 'boy', 'castro', 'damage', 'de', 'declared', 'distraught', 'estimated', 'flights', 'kits', 'lara', 'northwest', 'pair', 'plastic', 'rocked', 'sheeting', 'shovel', 'southeastern', 'swayed', 'touch', 'windows', 'acrobats', 'await', 'carmen', 'circus', 'dancers', 'ernesto', 'hands', 'hospital', 'lopez', 'marin', 'mountain', 'nearby', 'restored', 'southwest', 'struggling', 'surreal', 'terrible', 'touring', 'trees', 'unleashing', 'village', 'angeles', 'assist', 'blankets', 'blocks', 'businesses', 'causing', 'comfort', 'contributed', 'crying', 'danaher', 'deforestation', 'earthmovers', 'emilio', 'employed', 'excavations', 'firefighters', 'flakus', 'flattened', 'fought', 'generators', 'geological', 'hill', 'hillsides', 'homes', 'horrible', 'hour', 'impeding', 'landslides', 'located', 'loudspeaker', 'miami', 'performance', 'planeload', 'promote', 'renderos', 'ripped', 'roar', 'ronderos', 'slightly', 'slowed', 'stockpile', 'suitable', 'taiwan', 'tod', 'using', 'watchman', 'weeping', 'yard', 'advance', 'ages', 'alex', 'angel', 'appealing', 'architects', 'assessment', 'athe', 'aunt', 'aware', 'awe', 'ay', 'balsamo', 'barricade', 'bathroom', 'bicycles', 'bodyguard', 'boots', 'bracing', 'brick', 'bricks', 'buzz', 'ca', 'caked', 'cancelled', 'capable', 'car', 'churches', 'clerks', 'clod', 'cloud', 'columned', 'comasagua', 'containers', 'continuously', 'cooperated', 'coordinated', 'coordinating', 'cordillera', 'corners', 'corpse', 'cowboy', 'crawling', 'cries', 'crush', 'cry', 'curtains', 'dad', 'daughters', 'deaths', 'deforest', 'deforested', 'densely', 'desperation', 'destroyed', 'devastation', 'diggers', 'digging', 'disasters', 'dispersed', 'doorway', 'drifted', 'dug', 'dust', 'earthquake', 'eggs', 'electrician', 'employee', 'evacuating', 'exhausted', 'extract', 'fairly', 'feeding', 'ferrer', 'fierce', 'flat', 'florez', 'forests', 'franciscan', 'gaze', 'god', 'green', 'greg', 'grinding', 'grosance', 'grotesque', 'guadalupe', 'guatemalan', 'hammering', 'hello', 'hernandez', 'horribly', 'horses', 'household', 'houses', 'housewives', 'humbler', 'hunting', 'husky', 'hygiene', 'hymns', 'icon', 'impede', 'imperceptible', 'importer', 'incredible', 'intermittent']\n",
      "3 250 ['convicted', 'pardon', 'peltier', 'clemency', 'pardons', 'susan', 'influence', 'whitewater', 'activist', 'arkansas', 'clintons', 'jack', 'milken', 'pine', 'reservation', 'agents', 'junk', 'leonard', 'parole', 'ridge', 'ron', 'securities', 'dakota', 'espy', 'financier', 'freeh', 'native', 'agent', 'bond', 'commute', 'falsified', 'foods', 'guilty', 'jail', 'jonathan', 'mail', 'mcdougal', 'mike', 'schaffer', 'tyson', 'agricultural', 'banner', 'burkle', 'coler', 'conviction', 'dan', 'disdain', 'dougal', 'estate', 'felonies', 'freed', 'harbury', 'host', 'iii', 'king', 'koler', 'lloyd', 'loan', 'mandatory', 'marching', 'nonviolent', 'offenders', 'pardoning', 'rostenkowski', 'sentences', 'venture', 'violation', 'abhorrence', 'agriculture', 'angered', 'archie', 'arranging', 'associates', 'attracted', 'badges', 'benefactors', 'billions', 'birthday', 'bonds', 'boyfriend', 'briefly', 'cabin', 'chap', 'clergy', 'coats', 'concealed', 'correction', 'counts', 'date', 'davidian', 'defiantly', 'demonstrating', 'demonstrator', 'deserve', 'discipline', 'discretion', 'disrespect', 'dorothy', 'ellipse', 'expense', 'expressing', 'extradited', 'federally', 'felony', 'financing', 'fines', 'forget', 'fraud', 'fraudulent', 'gaines', 'gas', 'gunman', 'horiuchi', 'hubbell', 'idaho', 'inappropriate', 'inflicted', 'institution', 'interviewer', 'involvement', 'killings', 'leavenworth', 'leniency', 'list', 'marchers', 'marches', 'mcdougals', 'mercy', 'michael', 'mogul', 'murdering', 'original', 'paid', 'paramilitary', 'payment', 'pending', 'petition', 'pollard', 'predecessors', 'punishment', 'range', 'reflecting', 'regard', 'related', 'repaid', 'requests', 'resentment', 'restitution', 'review', 'ruby', 'sanctioned', 'savings', 'scarcely', 'sentence', 'sentenced', 'separate', 'separatist', 'settlements', 'shoot', 'slain', 'sole', 'steeped', 'street', 'supermarket', 'supervised', 'supporter', 'thevenot', 'unjust', 'unsuccessfully', 'vacation', 'webster', 'wields', 'workforce', 'abused', 'abuses', 'acceptance', 'acknowledges', 'acquitted', 'actions', 'active', 'admired', 'affair', 'aggressive', 'answer', 'applicants', 'archibald', 'artist', 'ashtrays', 'assailants', 'associate', 'associated', 'autonomy', 'ballistics', 'batch', 'blinked', 'boards', 'books', 'breakers', 'brokering', 'bulky', 'bullets', 'celebrity', 'chicago', 'choices', 'circuit', 'circumstance', 'cocaine', 'code', 'colleagues', 'commutation', 'commutations', 'commutes', 'compromise', 'comrades', 'confinement', 'connection', 'considering', 'contended', 'contributes', 'coretta', 'correct', 'cowling', 'creation', 'crime', 'criticizing', 'croifl', 'customary', 'defendant', 'defenders', 'defrauding', 'delegate', 'demonstration', 'deserved', 'diringer', 'disagrees', 'disposal', 'dollars', 'donating', 'drug', 'easing', 'edmundo', 'elated', 'elliot', 'emotional', 'engage', 'engaged', 'exemplified', 'exits', 'expenses', 'feats']\n",
      "4 250 ['australian', 'sampras', 'slam', 'tournament', 'champion', 'grand', 'agassi', 'andre', 'marat', 'safin', 'match', 'matches', 'pete', 'melbourne', 'player', 'seeded', 'davenport', 'ferrero', 'lindsay', 'played', 'title', 'titles', 'draw', 'game', 'gustavo', 'hingis', 'juan', 'kuerten', 'martina', 'monica', 'playing', 'seles', 'semifinals', 'tennis', 'venus', 'wimbledon', 'arena', 'bidding', 'elbow', 'kafelnikov', 'points', 'practicing', 'receiving', 'serena', 'sixth', 'spraining', 'tournaments', 'vodafone', 'yevgeny', 'aces', 'alun', 'ankle', 'antibiotics', 'arazi', 'baseline', 'beat', 'blanco', 'bothered', 'brie', 'classic', 'colonial', 'courts', 'daniela', 'defending', 'eliminated', 'favorable', 'feat', 'fever', 'fined', 'fourth', 'galo', 'glad', 'glue', 'hantuchova', 'henman', 'hicham', 'improvement', 'kournikova', 'liked', 'lorenzo', 'losing', 'magnus', 'mph', 'norman', 'optimistic', 'players', 'progressed', 'progresses', 'rafter', 'retractable', 'rippner', 'sandrine', 'singles', 'slovakia', 'sore', 'stadium', 'stafford', 'stickier', 'tanking', 'testud', 'trainer', 'twice', 'virus', 'advanced', 'anna', 'australia', 'beating', 'carlos', 'clijsters', 'cup', 'escude', 'finish', 'hewitt', 'improve', 'improved', 'jiri', 'lleyton', 'maria', 'ok', 'play', 'rankings', 'roof', 'sanchez', 'semis', 'starts', 'strange', 'sydney', 'tim', 'tricky', 'vanek', 'winning', 'abilities', 'achieving', 'anca', 'antonia', 'ball', 'barna', 'battling', 'breaks', 'coach', 'competitor', 'computer', 'crosscourt', 'doubles', 'errors', 'featured', 'forehand', 'gives', 'handle', 'kiefer', 'laver', 'masters', 'mild', 'nicolas', 'olympic', 'opens', 'park', 'racket', 'rackets', 'ranking', 'semifinal', 'streak', 'strokes', 'stumbled', 'sunny', 'surgery', 'tumbled', 'tuneup', 'victories', 'wild', 'winners', 'ace', 'add', 'admirer', 'amanda', 'amend', 'analyze', 'anke', 'annabel', 'annacone', 'anybody', 'ap', 'argentine', 'atp', 'attractive', 'aussie', 'australians', 'backhand', 'banned', 'belted', 'berths', 'betrayed', 'bother', 'bounced', 'bracket', 'brazil', 'brazilian', 'breast', 'brilliant', 'bryanne', 'caller', 'capability', 'capriati', 'championship', 'championships', 'chances', 'christen', 'christina', 'chunks', 'clash', 'clever', 'climbed', 'clinching', 'consistent', 'contend', 'cope', 'costa', 'couple', 'crest', 'cropped', 'damir', 'davis', 'decode', 'defend', 'definitely', 'defusing', 'dementieva', 'demystified', 'devilish', 'disgusted', 'dismal', 'dokic', 'dominance', 'dominate', 'dominikovic', 'dowse', 'dramatically', 'dwell', 'earned', 'elena', 'ellwood', 'encore', 'enjoy', 'enqvist', 'entertainer', 'evie', 'exasperated', 'exhibition', 'exited', 'expelled']\n",
      "5 250 ['india', 'pakistan', 'gujarat', 'delhi', 'epicenter', 'nepal', 'rubble', 'border', 'magnitude', 'panic', 'shaking', 'sway', 'ahmedabad', 'bhuj', 'bombay', 'desert', 'madras', 'parade', 'pradesh', 'tents', 'tremors', 'uttar', 'adoption', 'atal', 'bangladesh', 'devastating', 'ground', 'hyderabad', 'karachi', 'limbs', 'measuring', 'millions', 'multistory', 'panicked', 'rajasthan', 'slabs', 'subcontinent', 'surat', 'vajpayee', 'western', 'adult', 'affected', 'allahabad', 'apartment', 'arid', 'baijubahi', 'bihari', 'bleeding', 'bruised', 'calamity', 'calcutta', 'chill', 'climb', 'commemorates', 'cranes', 'crowded', 'donations', 'earthquakes', 'energy', 'entertainment', 'erie', 'exit', 'factory', 'festival', 'food', 'fremont', 'gupta', 'hallways', 'haren', 'hindu', 'hindus', 'holiday', 'identities', 'kumbh', 'lake', 'mahajan', 'mela', 'meteorological', 'musharraf', 'northwestern', 'ohio', 'overflowing', 'pakistani', 'pandya', 'patients', 'pervez', 'pilgrims', 'pondicherry', 'preliminary', 'quakes', 'represents', 'republic', 'satkhira', 'scientists', 'screamed', 'sifting', 'sindh', 'skyscrapers', 'stairwell', 'stampede', 'strongest', 'supply', 'swaying', 'temperatures', 'textile', 'trapped', 'uses', 'verandah', 'vineet', 'accepting', 'advani', 'adventist', 'aerial', 'aftershock', 'aftershocks', 'ahmadabad', 'airlift', 'alaskan', 'albany', 'alexandria', 'allen', 'americas', 'andreas', 'angles', 'angry', 'apply', 'arif', 'assam', 'assess', 'assessed', 'associations', 'atlanta', 'atom', 'atomic', 'attraction', 'avenue', 'awaited', 'az', 'baby', 'baltimore', 'barbara', 'barked', 'baskets', 'beautiful', 'behari', 'belch', 'beneath', 'besieged', 'bhaba', 'bhagat', 'bhat', 'bindra', 'birds', 'bleachers', 'bombs', 'bonfires', 'box', 'breakfast', 'broken', 'brunswick', 'buildings', 'builds', 'calamitous', 'calmed', 'campfires', 'camping', 'card', 'carolina', 'catholic', 'celebrates', 'celebrating', 'celeste', 'ceo', 'charleston', 'chasm', 'checking', 'choice', 'citing', 'click', 'collapsing', 'columbia', 'commissioner', 'common', 'communications', 'concentrate', 'concentrated', 'condolence', 'constructed', 'construction', 'contributions', 'coordinate', 'coordination', 'correspond', 'cow', 'cracks', 'crammed', 'crane', 'created', 'credit', 'crept', 'crevices', 'crust', 'cultural', 'cupboards', 'dc', 'desired', 'developed', 'dhingra', 'diamond', 'dignitaries', 'disrupted', 'distinctive', 'district', 'donated', 'donation', 'doorways', 'dot', 'dozens', 'dusty', 'dying', 'edges', 'efficiently', 'ellis', 'engineering', 'erupting', 'est', 'existent', 'extended', 'fairfax', 'fault', 'faye', 'fda', 'fear', 'fearing', 'feed', 'flexible', 'flooded', 'fully', 'ga', 'gandhinagar', 'gon', 'grain', 'graphically', 'greenway', 'grows', 'gujarati', 'gujral', 'halting', 'hardest', 'harm']\n",
      "6 250 ['convicts', 'escaped', 'sporting', 'goods', 'store', 'hawkins', 'eve', 'seven', 'texas', 'christmas', 'escape', 'escapees', 'irving', 'ammunition', 'connally', 'dallas', 'larry', 'officers', 'robbers', 'rounds', 'aubrey', 'kenedy', 'oshman', 'weapons', 'antonio', 'aubry', 'enforcement', 'fugitives', 'harper', 'holdup', 'marcos', 'pistols', 'rifle', 'rivas', 'shotgun', 'tull', 'gang', 'garcia', 'herded', 'leads', 'magnum', 'maximum', 'note', 'robbery', 'sketches', 'stabbed', 'stealing', 'stolen', 'weapon', 'aggravated', 'arsenal', 'assaulted', 'attempted', 'austin', 'blood', 'burglary', 'cannaday', 'clothes', 'colorado', 'corrections', 'criminal', 'donald', 'explorer', 'filed', 'fitzgerald', 'ford', 'guards', 'halprin', 'henry', 'inmates', 'inspector', 'jimmy', 'keith', 'kidnapping', 'loaded', 'lowell', 'makeshift', 'mcauliffe', 'murder', 'murphy', 'newbury', 'officer', 'perdue', 'ringleader', 'rodriguez', 'sexual', 'shooting', 'sighting', 'spotted', 'suspected', 'tied', 'warrants', 'whereabouts', 'alarm', 'anthony', 'arlington', 'assistant', 'automatic', 'bicycle', 'burglar', 'capacity', 'cash', 'casing', 'convinced', 'customers', 'dangerous', 'deadly', 'driving', 'durango', 'employees', 'escapes', 'escaping', 'ethan', 'fingerprints', 'firearms', 'guys', 'heavily', 'lax', 'lip', 'louisiana', 'maintenance', 'male', 'oklahoma', 'overpowered', 'photos', 'pizza', 'policeman', 'posted', 'proven', 'randy', 'robbing', 'severely', 'sharpened', 'shellee', 'sightings', 'simultaneous', 'sports', 'suspicious', 'tips', 'todd', 'unit', 'updated', 'utility', 'vehicle', 'website', 'abuser', 'access', 'accidental', 'acted', 'adorned', 'agree', 'alarming', 'alcohol', 'altered', 'amassed', 'ammo', 'analyzing', 'answers', 'appearances', 'approached', 'approaches', 'arizona', 'armed', 'asks', 'assault', 'autopsy', 'bandits', 'bank', 'beretta', 'blaming', 'bold', 'bored', 'boxes', 'brash', 'breaking', 'breakout', 'brutally', 'bulletproof', 'busted', 'buzzing', 'camera', 'carver', 'cashier', 'catch', 'catching', 'cautiously', 'chapel', 'child', 'chips', 'circulate', 'colony', 'comb', 'compared', 'concealment', 'cop', 'corner', 'cornered', 'countryside', 'craig', 'criminals', 'cruisers', 'cunning', 'dalworthington', 'daniels', 'daring', 'dated', 'daylight', 'departments', 'depth', 'descriptions', 'dinner', 'disguise', 'dispatcher', 'dispose', 'distinct', 'domino', 'drowned', 'duct', 'enters', 'entice', 'escapee', 'escort', 'everywhere', 'excitement', 'excuses', 'facial', 'falls', 'features', 'files', 'fireworks', 'floyd', 'footage', 'fracture', 'frankly', 'fremd', 'fruitful', 'funny', 'galvin', 'gardens', 'garland', 'gates', 'georgia', 'getaway', 'glasses', 'gloria', 'grieving', 'griffith', 'gun', 'gunfire', 'gunned']\n",
      "7 250 ['torpedo', 'pope', 'cancer', 'edmund', 'secret', 'lawyer', 'bone', 'businessman', 'buy', 'trial', 'edmond', 'espionage', 'moscow', 'remission', 'testimony', 'arrested', 'clinic', 'examination', 'health', 'lefortovo', 'proceedings', 'purchase', 'spy', 'astakhov', 'client', 'courtroom', 'denies', 'diagnosed', 'doctor', 'illegally', 'innocent', 'lunch', 'material', 'openly', 'pavel', 'propulsion', 'research', 'accuses', 'adjourned', 'ailing', 'blueprints', 'cerf', 'conducted', 'doubled', 'expert', 'expertise', 'fit', 'founded', 'fsb', 'inflammation', 'joints', 'kgb', 'nina', 'objections', 'scientist', 'sold', 'specializing', 'technical', 'technologies', 'torpedoes', 'untreated', 'accusers', 'acquittal', 'adequate', 'adjournment', 'admen', 'afterwards', 'allege', 'allegedly', 'ample', 'appeals', 'arrange', 'arrest', 'arseni', 'arthritis', 'attempting', 'babkin', 'bar', 'barkina', 'barkolla', 'barkova', 'bauman', 'biased', 'boasting', 'boucher', 'bout', 'bouts', 'bowmen', 'businessmen', 'championing', 'cheri', 'cited', 'claiming', 'classified', 'coat', 'coller', 'conduct', 'corresponding', 'culture', 'declassified', 'declassify', 'defenses', 'delayed', 'denying', 'detention', 'diagnosis', 'distributed', 'doors', 'edwin', 'embrace', 'employing', 'enable', 'endangers', 'entirely', 'establish', 'eversince', 'everyday', 'examine', 'fadeyev', 'fresh', 'furnish', 'garry', 'georgy', 'gifts', 'gloves', 'granted', 'grounds', 'harmful', 'hip', 'ill', 'implicated', 'improperly', 'incarcerated', 'informed', 'interfax', 'interpreter', 'ivashkov', 'joint', 'joke', 'jury', 'lacked', 'lefrotovo', 'letters', 'lisa', 'logvinovich', 'manufacturing', 'maritime', 'marrow', 'medication', 'medicines', 'menet', 'mianded', 'moralis', 'motions', 'nena', 'nened', 'obsession', 'obtained', 'obtaining', 'oleg', 'opportunities', 'orbit', 'oskinoff', 'osteochondrosis', 'padella', 'pain', 'parvail', 'pavals', 'pavelo', 'pennsylvania', 'pivotal', 'plea', 'plotnikov', 'poor', 'pretend', 'pri', 'prints', 'procedure', 'prope', 'prosecuting', 'prosecutions', 'prosecutor', 'prosecutors', 'reality', 'recanted', 'recuperating', 'relating', 'reminisce', 'requested', 'resurfaced', 'resurfacing', 'retracted', 'rheumatic', 'scheveck', 'schlikov', 'secrecy', 'secrets', 'severe', 'shebeck', 'sherry', 'shkval', 'spoken', 'spying', 'standards', 'sticking', 'stokovs', 'strakovs', 'suggestions', 'summary', 'symptoms', 'technology', 'temperature', 'testified', 'testing', 'tokov', 'tolly', 'trailed', 'ultra', 'ultramodern', 'unacceptable', 'valdimir', 'vellary', 'visa', 'vitaly', 'witness', 'worlds', 'yuri', 'allegation', 'allow', 'allowed', 'applied', 'blatant', 'blue', 'captured', 'closely', 'college', 'conducting', 'connected', 'consular', 'demand', 'designs', 'developing', 'discussed', 'discussions', 'doctors', 'drama', 'examined', 'flaring', 'grandson', 'hopeful']\n",
      "8 250 ['cole', 'terrorism', 'port', 'uss', 'yemen', 'aden', 'terrorist', 'peninsula', 'refueling', 'accountable', 'arabian', 'boat', 'bombing', 'cowardly', 'despicable', 'pentagon', 'responsible', 'ship', 'suicide', 'act', 'deter', 'docked', 'fail', 'israelis', 'utterly', 'alongside', 'attack', 'injuring', 'intention', 'mission', 'palestinian', 'palestinians', 'promoting', 'ships', 'tore', 'appears', 'arafat', 'bahrain', 'casualties', 'degrees', 'flooding', 'gulf', 'heightened', 'listing', 'middle', 'norfolk', 'notified', 'persian', 'rammed', 'yasser', 'aegis', 'alert', 'apparent', 'barak', 'bases', 'burke', 'canal', 'carries', 'chappaqua', 'cmdr', 'cohen', 'contained', 'daren', 'east', 'ehud', 'fires', 'fuel', 'israel', 'israeli', 'manama', 'pelkie', 'pledged', 'ram', 'rip', 'sailor', 'sparked', 'suez', 'territories', 'triggered', 'ultimately', 'weaponry', 'abdallah', 'addition', 'alternative', 'ambulances', 'anguish', 'arriving', 'bombings', 'casualty', 'centers', 'clashes', 'confronting', 'consulting', 'dialogue', 'dock', 'explosives', 'garden', 'harbor', 'hypothetical', 'installations', 'justification', 'marine', 'missiles', 'mob', 'mooring', 'ordered', 'outbreak', 'packed', 'precaution', 'professional', 'ramallah', 'refueled', 'registry', 'reno', 'responsibility', 'retaliated', 'retaliation', 'rushing', 'sophisticated', 'tearing', 'transited', 'undertake', 'unfolding', 'x', 'yemeni', 'afghanistan', 'ahmed', 'airmen', 'alerted', 'alerting', 'ali', 'allows', 'alpha', 'annan', 'antenna', 'anwar', 'arabia', 'arkin', 'armor', 'ashore', 'assassinated', 'attaches', 'attacked', 'attackers', 'attacking', 'attacks', 'battened', 'berger', 'bin', 'blackened', 'blair', 'blast', 'blew', 'boats', 'boldness', 'bombed', 'bordering', 'boulevard', 'brig', 'broadcasting', 'buds', 'bulletins', 'buoy', 'burch', 'cameras', 'carlton', 'carr', 'carrier', 'catherine', 'caution', 'celebrated', 'chad', 'claimed', 'commissioned', 'commonly', 'communicate', 'con', 'conclusion', 'condemn', 'contacts', 'counselors', 'craft', 'culprits', 'daily', 'darrell', 'debated', 'declined', 'demonstrations', 'depicts', 'deplore', 'deplores', 'deployed', 'deployment', 'deployments', 'depress', 'desperately', 'destructiveness', 'detailed', 'dhahran', 'dispatch', 'dispatched', 'dispatching', 'due', 'eagles', 'edt', 'egyptian', 'eleven', 'embassies', 'emphasized', 'escalation', 'everybody', 'exact', 'exchanges', 'extra', 'exulting', 'eyes', 'female', 'fights', 'firmly', 'fleet', 'fortified', 'freely', 'fueling', 'gaping', 'garage', 'gash', 'gaza', 'globe', 'golfers', 'goodbye', 'grace', 'guests', 'hamas', 'hampton', 'harry', 'heroism', 'hezbollah', 'homecoming', 'hub', 'hugging', 'hunters', 'husbands', 'hysterical', 'inflatable', 'informers', 'instantly', 'insurance', 'investigate', 'investigative', 'iwo']\n",
      "9 250 ['ivory', 'guei', 'african', 'alassane', 'coup', 'ouattara', 'abidjan', 'ruling', 'dramane', 'junta', 'cocoa', 'coffee', 'excluded', 'gbagbo', 'ivorian', 'laurent', 'exports', 'henri', 'suit', 'tie', 'bedie', 'candidacy', 'exclusion', 'konan', 'supreme', 'uniform', 'africa', 'december', 'parties', 'requires', 'boycott', 'burkina', 'campaigning', 'corrupt', 'coulibaly', 'countered', 'curfew', 'dependent', 'faso', 'fearful', 'handover', 'initially', 'ivorians', 'korhogo', 'mutinies', 'origin', 'ousted', 'prices', 'primary', 'rallied', 'reprisals', 'seizing', 'suffer', 'traded', 'unity', 'unrest', 'withdrawn', 'alike', 'barred', 'bids', 'boycotting', 'civilians', 'dear', 'decline', 'dialo', 'disqualified', 'drissa', 'eligible', 'emile', 'funding', 'insists', 'investors', 'nonexistent', 'pdci', 'rdr', 'republicans', 'serenity', 'tranquility', 'twenty', 'ado', 'albeit', 'assassination', 'assembled', 'bandannas', 'battered', 'bema', 'bombet', 'burn', 'buses', 'camped', 'candidates', 'casting', 'cede', 'chanting', 'cheering', 'competition', 'constant', 'contenders', 'defiant', 'divided', 'divisive', 'doubts', 'elect', 'entrench', 'erosion', 'ethnically', 'explode', 'favor', 'fewer', 'fofana', 'harsh', 'history', 'hulking', 'incapable', 'instability', 'instructing', 'kone', 'longtime', 'mamadou', 'meets', 'monetary', 'muted', 'nassoue', 'nickname', 'nicodeme', 'oppose', 'origins', 'ouster', 'parents', 'politician', 'polls', 'preserve', 'professor', 'qualifications', 'reinforced', 'resist', 'robert', 'sandbag', 'shreds', 'soldier', 'sparking', 'stores', 'submitted', 'subverting', 'sweep', 'thrown', 'tia', 'trickled', 'truly', 'turnout', 'usual', 'voter', 'wane', 'winter', 'zan', 'abou', 'adjame', 'allegations', 'aly', 'anger', 'approved', 'arrests', 'assassinate', 'assumed', 'attached', 'attributed', 'barbed', 'barracks', 'barriers', 'blaring', 'blemished', 'bodyguards', 'bomber', 'bombey', 'cesar', 'cisse', 'climate', 'cocody', 'compensate', 'competing', 'conciliatory', 'contends', 'converged', 'counterattack', 'crises', 'custody', 'defaced', 'delays', 'delivered', 'denounced', 'deposit', 'destabilize', 'deteriorate', 'device', 'dioulo', 'disaffected', 'disorder', 'dominique', 'dow', 'driver', 'eg', 'eligibility', 'empty', 'enforced', 'escorting', 'failure', 'famed', 'filled', 'flimsy', 'forbidden', 'foreigners', 'fpi', 'francis', 'functioning', 'governing', 'hanging', 'harass', 'harassing', 'hardening', 'heavyweight', 'hobbled', 'hung', 'ignored', 'inability', 'ineligible', 'initiate', 'intellectual', 'interested', 'jousts', 'kidnap', 'laborer', 'lacina', 'lanviara', 'legitimate', 'liveliest', 'lust', 'mali', 'malice', 'managing', 'mandated', 'masquerade', 'motorbikes', 'moves', 'multiparty', 'nancy', 'neighborhoods', 'nemesis', 'newswires', 'nighttime', 'northerners']\n",
      "10 250 ['carnahan', 'governor', 'mel', 'plane', 'crash', 'ashcroft', 'chris', 'jefferson', 'republican', 'sifford', 'louis', 'roger', 'cessna', 'madrid', 'county', 'democrat', 'gov', 'hilly', 'jean', 'lieutenant', 'dive', 'engine', 'jerry', 'mourners', 'piloting', 'race', 'radar', 'rolla', 'seat', 'senate', 'sounded', 'wilson', 'wooded', 'wreckage', 'appoint', 'ballot', 'condolences', 'crashed', 'crazy', 'debate', 'error', 'flying', 'foggy', 'frank', 'governors', 'hunter', 'kemp', 'kmov', 'lightning', 'litton', 'municipal', 'nachtigal', 'ntsb', 'rainy', 'remainder', 'sheriff', 'silence', 'sky', 'tom', 'transportation', 'treasurer', 'aide', 'airplane', 'cahokia', 'campaigns', 'carmody', 'carol', 'celebration', 'chiles', 'contested', 'deadline', 'debates', 'democrats', 'ed', 'encountered', 'formal', 'format', 'gore', 'gymnasium', 'gyroscope', 'heat', 'hometown', 'howard', 'instruments', 'iowa', 'lawton', 'loud', 'mickelson', 'mideast', 'mourning', 'nomination', 'overhead', 'practice', 'races', 'rain', 'registered', 'screaming', 'senator', 'shultz', 'silo', 'slammed', 'susie', 'acting', 'advertising', 'aides', 'al', 'audience', 'aviation', 'band', 'banking', 'bekki', 'black', 'brown', 'campaigned', 'campus', 'cancellation', 'capitol', 'casket', 'cast', 'climbing', 'closest', 'comfortable', 'compelling', 'constituents', 'cory', 'daley', 'dearly', 'dedicated', 'demeanor', 'discussion', 'distinguished', 'dwindling', 'editor', 'egypt', 'elizabeth', 'encounter', 'ends', 'excellence', 'exercising', 'farm', 'feature', 'fiercely', 'fill', 'flight', 'flowers', 'gallup', 'gyro', 'hailed', 'honestly', 'horizon', 'hotly', 'informal', 'informally', 'innsbrook', 'instrument', 'investigator', 'inviting', 'isham', 'kansas', 'lambert', 'lapel', 'laura', 'lehrer', 'licensed', 'lieberman', 'mansion', 'margin', 'marks', 'mindful', 'mississippi', 'mock', 'moderator', 'mums', 'newport', 'noble', 'objected', 'onlookers', 'orientation', 'pall', 'parks', 'pbs', 'perch', 'pilot', 'pilots', 'postponement', 'praised', 'previewing', 'quality', 'questioners', 'random', 'receives', 'recruit', 'regular', 'replacement', 'retake', 'revving', 'roy', 'saddened', 'sample', 'screened', 'shock', 'shuttling', 'stakes', 'stools', 'stretch', 'stumble', 'suburbs', 'temple', 'thoughtful', 'uncharacteristically', 'uncommitted', 'undecided', 'vacancy', 'vacant', 'waking', 'whenever', 'wonderful', 'wooing', 'zogby', 'aaron', 'accidents', 'additional', 'ads', 'advisers', 'age', 'aisle', 'altitude', 'amateur', 'amp', 'arrivals', 'artificial', 'ashkroft', 'attending', 'ayres', 'backup', 'bartlett', 'beloved', 'blamed', 'bloc', 'bob', 'calvert', 'campaign', 'candles', 'challenger', 'challenging', 'changing', 'chillicothe', 'chrysanthemums', 'claire', 'clearer']\n",
      "11 250 ['alberto', 'fujimori', 'peru', 'peruvian', 'resign', 'resignation', 'asylum', 'japan', 'scandal', 'vladimiro', 'federico', 'marquez', 'panama', 'ricardo', 'salas', 'scandals', 'tokyo', 'democratization', 'ending', 'fugitive', 'lima', 'montesinos', 'paniagua', 'radioprogramas', 'transparent', 'valentin', 'absolutely', 'leaked', 'obstacle', 'remove', 'tudela', 'ancestral', 'announcement', 'authoritarian', 'bribing', 'loans', 'presented', 'quit', 'trafficking', 'aritomi', 'complained', 'congressman', 'corruption', 'cotton', 'detour', 'directing', 'dragged', 'ease', 'explanation', 'hiro', 'homeland', 'incapacity', 'maintained', 'measures', 'moral', 'picked', 'resigns', 'sister', 'tailor', 'unannounced', 'undercut', 'videotape', 'alejandro', 'assume', 'bid', 'complaints', 'criticized', 'crumbled', 'decade', 'defended', 'discovered', 'embattled', 'fernando', 'froze', 'grip', 'illegitimate', 'immigrant', 'latin', 'laundering', 'lawmakers', 'layover', 'marred', 'moderate', 'narcotics', 'negotiate', 'olivera', 'permitted', 'post', 'profits', 'prolonged', 'proved', 'rebels', 'resigning', 'revolving', 'shop', 'skimming', 'spymaster', 'squads', 'succession', 'swiss', 'toledo', 'torture', 'trades', 'unsuccessful', 'video', 'absence', 'accepted', 'accounts', 'administering', 'alleging', 'amid', 'amount', 'april', 'autocratic', 'boycotted', 'bribe', 'canceling', 'contact', 'corresponds', 'countryman', 'cpn', 'credentials', 'demise', 'denials', 'dirty', 'echoing', 'effective', 'ellen', 'ensure', 'facing', 'foes', 'forced', 'formalize', 'handed', 'hisashi', 'hyperinflation', 'inform', 'installed', 'insure', 'intimidation', 'irresponsible', 'jorge', 'lawmaker', 'leftist', 'legislators', 'mafia', 'malaysian', 'mary', 'masterminded', 'mistaken', 'montecinos', 'oas', 'ombudsman', 'planes', 'proceeds', 'ranging', 'rulers', 'rumors', 'santistevan', 'signaling', 'slate', 'smooth', 'smoothly', 'succumbing', 'swelled', 'traffickers', 'tricks', 'tumultuous', 'ueno', 'wiped', 'absentee', 'advisor', 'announcements', 'announcing', 'anticipated', 'anymore', 'apart', 'appearance', 'argument', 'arguments', 'arrangement', 'bavaria', 'blakemore', 'bolona', 'boost', 'born', 'bowen', 'brief', 'briskly', 'broadly', 'budget', 'cable', 'cayman', 'centrist', 'channel', 'chaos', 'chaotic', 'character', 'circuitous', 'clifford', 'coca', 'cold', 'colombian', 'comeback', 'comments', 'communiqu', 'comply', 'compromised', 'concerted', 'confirming', 'congress', 'context', 'contradicted', 'controlling', 'cooperate', 'counties', 'crumbling', 'cuba', 'cultivation', 'dealings', 'defeating', 'deficit', 'defied', 'definition', 'definitive', 'democratize', 'departure', 'deportation', 'deportieren', 'dictatorial', 'digits', 'disenfranchise', 'dispel', 'downtown', 'dropping', 'dumping', 'ecuador', 'editors', 'elaborate', 'element', 'emerged', 'enact', 'endorsed', 'enmeshed', 'eroded', 'et', 'explain', 'explaining', 'fairness', 'fatigue']\n",
      "12 250 ['hanoi', 'vietnamese', 'china', 'communist', 'ho', 'saigon', 'asia', 'chi', 'minh', 'relationship', 'unified', 'visit', 'rice', 'trade', 'agreement', 'business', 'ceremony', 'chelsea', 'commerce', 'farewell', 'jiang', 'korean', 'markets', 'mori', 'paddy', 'relations', 'visits', 'yoshiro', 'zemin', 'action', 'borneo', 'chinese', 'continues', 'diplomatic', 'embargo', 'excavation', 'formerly', 'hawaii', 'internet', 'lifted', 'nixon', 'servicemen', 'successor', 'sultanate', 'surrendered', 'accounting', 'benefits', 'bilateral', 'broadcast', 'brunei', 'chapter', 'christopher', 'cody', 'crossing', 'doubt', 'draft', 'emigrate', 'evert', 'generally', 'growth', 'hillary', 'jet', 'lawrence', 'learned', 'mystery', 'negotiations', 'nguyen', 'pows', 'proposition', 'pursue', 'quarter', 'reform', 'repatriation', 'sidelines', 'society', 'spending', 'troops', 'unaccounted', 'undertaken', 'unfettered', 'vigorous', 'warren', 'accompanied', 'accounted', 'agenda', 'agonizing', 'allowing', 'ann', 'anthropologist', 'anthropologists', 'apec', 'apology', 'archaeological', 'assured', 'beach', 'beginning', 'bones', 'bunch', 'bureaucracy', 'cambodia', 'cancel', 'captive', 'caucasians', 'century', 'closure', 'clue', 'commitments', 'confounded', 'conjunction', 'contain', 'costly', 'crashes', 'crater', 'culmination', 'customs', 'daughter', 'deny', 'digs', 'discrepancies', 'divisions', 'duties', 'dy', 'easier', 'echo', 'empire', 'enemy', 'exist', 'expanded', 'expensive', 'fighter', 'fills', 'flow', 'focusing', 'forensics', 'freedoms', 'fullest', 'future', 'gene', 'global', 'highlight', 'hinges', 'hoes', 'hollywood', 'identification', 'impasse', 'importance', 'indochina', 'integrated', 'interest', 'investment', 'itinerary', 'johnson', 'joked', 'kim', 'korea', 'laborers', 'laos', 'le', 'liberalization', 'lightheartedly', 'lingering', 'listed', 'longstanding', 'lyndon', 'marking', 'mccain', 'mia', 'mias', 'moratorium', 'mostly', 'motorcade', 'mounds', 'movies', 'nien', 'normalizing', 'occasionally', 'opposed', 'opulent', 'oversees', 'owes', 'painstakingly', 'pays', 'portion', 'predictions', 'prelude', 'program', 'prosperity', 'protester', 'puzzle', 'rancor', 'rapprochement', 'reaches', 'reaffirmed', 'regain', 'reinforcing', 'reiterated', 'remotest', 'reopened', 'repatriated', 'resolve', 'restrictions', 'revealing', 'richard', 'rim', 'rodham', 'roughly', 'sees', 'select', 'sieves', 'sift', 'sliver', 'societies', 'spend', 'sperling', 'stepped', 'stony', 'stressed', 'struggled', 'suspense', 'sweeping', 'symbolic', 'talks', 'trip', 'turnarounds', 'unconditionally', 'unresolved', 'unshaken', 'urgency', 'useful', 'waiver', 'wartime', 'washed', 'wielding', 'withdrawal', 'withheld', 'acceded', 'accompanying', 'accomplish', 'accords', 'achieved', 'admiringly', 'advice', 'aging', 'agrees', 'airline', 'akin', 'alignment', 'ambivalent', 'amerasian', 'amounts']\n"
     ]
    }
   ],
   "source": [
    "# Select the top 500 features for every class\n",
    "\n",
    "FeatureSelected = []\n",
    "tmp = []\n",
    "limit = 250\n",
    "\n",
    "for i in range(13):\n",
    "    tup = []\n",
    "    for j in range(len(ClassifiedDictionary)):\n",
    "        tup.append((ClassifiedDictionary[j],LikelihoodRatio[i][j]))\n",
    "    tup.sort(key=lambda tup: tup[1])  # sorts in place\n",
    "    tmp.append(tup)\n",
    "# print(tmp[12])\n",
    "\n",
    "for i in range(13):\n",
    "    tmp2 = []\n",
    "    for j in range(limit):\n",
    "        tmp2.append(tmp[i][j][0])\n",
    "    FeatureSelected.append(tmp2)\n",
    "        \n",
    "for j in range(13):\n",
    "    print(j,len(FeatureSelected[j]),FeatureSelected[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index 1095 documents\n",
    "\n",
    "RawDocument = []\n",
    "a = []\n",
    "b = 0\n",
    "FileNum = 1095 #先用當5代表\n",
    "\n",
    "for i in range (FileNum):\n",
    "    b = b+1\n",
    "    a.append(str(b) + '.txt')\n",
    "#     print (a[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poter's Algorithm\n",
    "\n",
    "ClassifiedDocument = []\n",
    "\n",
    "class PorterStemmer:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.b = \"\"  # buffer for word to be stemmed\n",
    "        self.k = 0\n",
    "        self.k0 = 0\n",
    "        self.j = 0   # j is a general offset into the string\n",
    "\n",
    "    def cons(self, i):\n",
    "        if self.b[i] == 'a' or self.b[i] == 'e' or self.b[i] == 'i' or self.b[i] == 'o' or self.b[i] == 'u':\n",
    "            return 0\n",
    "        if self.b[i] == 'y':\n",
    "            if i == self.k0:\n",
    "                return 1\n",
    "            else:\n",
    "                return (not self.cons(i - 1))\n",
    "        return 1\n",
    "\n",
    "    def m(self):\n",
    "        n = 0\n",
    "        i = self.k0\n",
    "        while 1:\n",
    "            if i > self.j:\n",
    "                return n\n",
    "            if not self.cons(i):\n",
    "                break\n",
    "            i = i + 1\n",
    "        i = i + 1\n",
    "        while 1:\n",
    "            while 1:\n",
    "                if i > self.j:\n",
    "                    return n\n",
    "                if self.cons(i):\n",
    "                    break\n",
    "                i = i + 1\n",
    "            i = i + 1\n",
    "            n = n + 1\n",
    "            while 1:\n",
    "                if i > self.j:\n",
    "                    return n\n",
    "                if not self.cons(i):\n",
    "                    break\n",
    "                i = i + 1\n",
    "            i = i + 1\n",
    "\n",
    "    def vowelinstem(self):\n",
    "        for i in range(self.k0, self.j + 1):\n",
    "            if not self.cons(i):\n",
    "                return 1\n",
    "        return 0\n",
    "\n",
    "    def doublec(self, j):\n",
    "        if j < (self.k0 + 1):\n",
    "            return 0\n",
    "        if (self.b[j] != self.b[j-1]):\n",
    "            return 0\n",
    "        return self.cons(j)\n",
    "\n",
    "    def cvc(self, i):\n",
    "        if i < (self.k0 + 2) or not self.cons(i) or self.cons(i-1) or not self.cons(i-2):\n",
    "            return 0\n",
    "        ch = self.b[i]\n",
    "        if ch == 'w' or ch == 'x' or ch == 'y':\n",
    "            return 0\n",
    "        return 1\n",
    "\n",
    "    def ends(self, s):\n",
    "        length = len(s)\n",
    "        if s[length - 1] != self.b[self.k]: # tiny speed-up\n",
    "            return 0\n",
    "        if length > (self.k - self.k0 + 1):\n",
    "            return 0\n",
    "        if self.b[self.k-length+1:self.k+1] != s:\n",
    "            return 0\n",
    "        self.j = self.k - length\n",
    "        return 1\n",
    "\n",
    "    def setto(self, s):\n",
    "        length = len(s)\n",
    "        self.b = self.b[:self.j+1] + s + self.b[self.j+length+1:]\n",
    "        self.k = self.j + length\n",
    "\n",
    "    def r(self, s):\n",
    "        if self.m() > 0:\n",
    "            self.setto(s)\n",
    "\n",
    "    def step1ab(self):\n",
    "        \"\"\"step1ab() gets rid of plurals and -ed or -ing. e.g.\n",
    "\n",
    "           caresses  ->  caress\n",
    "           ponies    ->  poni\n",
    "           ties      ->  ti\n",
    "           caress    ->  caress\n",
    "           cats      ->  cat\n",
    "\n",
    "           feed      ->  feed\n",
    "           agreed    ->  agree\n",
    "           disabled  ->  disable\n",
    "\n",
    "           matting   ->  mat\n",
    "           mating    ->  mate\n",
    "           meeting   ->  meet\n",
    "           milling   ->  mill\n",
    "           messing   ->  mess\n",
    "\n",
    "           meetings  ->  meet\n",
    "        \"\"\"\n",
    "        if self.b[self.k] == 's':\n",
    "            if self.ends(\"sses\"):\n",
    "                self.k = self.k - 2\n",
    "            elif self.ends(\"ies\"):\n",
    "                self.setto(\"i\")\n",
    "            elif self.b[self.k - 1] != 's':\n",
    "                self.k = self.k - 1\n",
    "        if self.ends(\"eed\"):\n",
    "            if self.m() > 0:\n",
    "                self.k = self.k - 1\n",
    "        elif (self.ends(\"ed\") or self.ends(\"ing\")) and self.vowelinstem():\n",
    "            self.k = self.j\n",
    "            if self.ends(\"at\"):   self.setto(\"ate\")\n",
    "            elif self.ends(\"bl\"): self.setto(\"ble\")\n",
    "            elif self.ends(\"iz\"): self.setto(\"ize\")\n",
    "            elif self.doublec(self.k):\n",
    "                self.k = self.k - 1\n",
    "                ch = self.b[self.k]\n",
    "                if ch == 'l' or ch == 's' or ch == 'z':\n",
    "                    self.k = self.k + 1\n",
    "            elif (self.m() == 1 and self.cvc(self.k)):\n",
    "                self.setto(\"e\")\n",
    "\n",
    "    def step1c(self):\n",
    "        \"\"\"step1c() turns terminal y to i when there is another vowel in the stem.\"\"\"\n",
    "        if (self.ends(\"y\") and self.vowelinstem()):\n",
    "            self.b = self.b[:self.k] + 'i' + self.b[self.k+1:]\n",
    "\n",
    "    def step2(self):\n",
    "        \"\"\"step2() maps double suffices to single ones.\n",
    "        so -ization ( = -ize plus -ation) maps to -ize etc. note that the\n",
    "        string before the suffix must give m() > 0.\n",
    "        \"\"\"\n",
    "        if self.b[self.k - 1] == 'a':\n",
    "            if self.ends(\"ational\"):   self.r(\"ate\")\n",
    "            elif self.ends(\"tional\"):  self.r(\"tion\")\n",
    "        elif self.b[self.k - 1] == 'c':\n",
    "            if self.ends(\"enci\"):      self.r(\"ence\")\n",
    "            elif self.ends(\"anci\"):    self.r(\"ance\")\n",
    "        elif self.b[self.k - 1] == 'e':\n",
    "            if self.ends(\"izer\"):      self.r(\"ize\")\n",
    "        elif self.b[self.k - 1] == 'l':\n",
    "            if self.ends(\"bli\"):       self.r(\"ble\") # --DEPARTURE--\n",
    "            # To match the published algorithm, replace this phrase with\n",
    "            #   if self.ends(\"abli\"):      self.r(\"able\")\n",
    "            elif self.ends(\"alli\"):    self.r(\"al\")\n",
    "            elif self.ends(\"entli\"):   self.r(\"ent\")\n",
    "            elif self.ends(\"eli\"):     self.r(\"e\")\n",
    "            elif self.ends(\"ousli\"):   self.r(\"ous\")\n",
    "        elif self.b[self.k - 1] == 'o':\n",
    "            if self.ends(\"ization\"):   self.r(\"ize\")\n",
    "            elif self.ends(\"ation\"):   self.r(\"ate\")\n",
    "            elif self.ends(\"ator\"):    self.r(\"ate\")\n",
    "        elif self.b[self.k - 1] == 's':\n",
    "            if self.ends(\"alism\"):     self.r(\"al\")\n",
    "            elif self.ends(\"iveness\"): self.r(\"ive\")\n",
    "            elif self.ends(\"fulness\"): self.r(\"ful\")\n",
    "            elif self.ends(\"ousness\"): self.r(\"ous\")\n",
    "        elif self.b[self.k - 1] == 't':\n",
    "            if self.ends(\"aliti\"):     self.r(\"al\")\n",
    "            elif self.ends(\"iviti\"):   self.r(\"ive\")\n",
    "            elif self.ends(\"biliti\"):  self.r(\"ble\")\n",
    "        elif self.b[self.k - 1] == 'g': # --DEPARTURE--\n",
    "            if self.ends(\"logi\"):      self.r(\"log\")\n",
    "        # To match the published algorithm, delete this phrase\n",
    "\n",
    "    def step3(self):\n",
    "        \"\"\"step3() dels with -ic-, -full, -ness etc. similar strategy to step2.\"\"\"\n",
    "        if self.b[self.k] == 'e':\n",
    "            if self.ends(\"icate\"):     self.r(\"ic\")\n",
    "            elif self.ends(\"ative\"):   self.r(\"\")\n",
    "            elif self.ends(\"alize\"):   self.r(\"al\")\n",
    "        elif self.b[self.k] == 'i':\n",
    "            if self.ends(\"iciti\"):     self.r(\"ic\")\n",
    "        elif self.b[self.k] == 'l':\n",
    "            if self.ends(\"ical\"):      self.r(\"ic\")\n",
    "            elif self.ends(\"ful\"):     self.r(\"\")\n",
    "        elif self.b[self.k] == 's':\n",
    "            if self.ends(\"ness\"):      self.r(\"\")\n",
    "\n",
    "    def step4(self):\n",
    "        \"\"\"step4() takes off -ant, -ence etc., in context <c>vcvc<v>.\"\"\"\n",
    "        if self.b[self.k - 1] == 'a':\n",
    "            if self.ends(\"al\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 'c':\n",
    "            if self.ends(\"ance\"): pass\n",
    "            elif self.ends(\"ence\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 'e':\n",
    "            if self.ends(\"er\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 'i':\n",
    "            if self.ends(\"ic\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 'l':\n",
    "            if self.ends(\"able\"): pass\n",
    "            elif self.ends(\"ible\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 'n':\n",
    "            if self.ends(\"ant\"): pass\n",
    "            elif self.ends(\"ement\"): pass\n",
    "            elif self.ends(\"ment\"): pass\n",
    "            elif self.ends(\"ent\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 'o':\n",
    "            if self.ends(\"ion\") and (self.b[self.j] == 's' or self.b[self.j] == 't'): pass\n",
    "            elif self.ends(\"ou\"): pass\n",
    "            # takes care of -ous\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 's':\n",
    "            if self.ends(\"ism\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 't':\n",
    "            if self.ends(\"ate\"): pass\n",
    "            elif self.ends(\"iti\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 'u':\n",
    "            if self.ends(\"ous\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 'v':\n",
    "            if self.ends(\"ive\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 'z':\n",
    "            if self.ends(\"ize\"): pass\n",
    "            else: return\n",
    "        else:\n",
    "            return\n",
    "        if self.m() > 1:\n",
    "            self.k = self.j\n",
    "\n",
    "    def step5(self):\n",
    "        \"\"\"step5() removes a final -e if m() > 1, and changes -ll to -l if\n",
    "        m() > 1.\n",
    "        \"\"\"\n",
    "        self.j = self.k\n",
    "        if self.b[self.k] == 'e':\n",
    "            a = self.m()\n",
    "            if a > 1 or (a == 1 and not self.cvc(self.k-1)):\n",
    "                self.k = self.k - 1\n",
    "        if self.b[self.k] == 'l' and self.doublec(self.k) and self.m() > 1:\n",
    "            self.k = self.k -1\n",
    "\n",
    "    def stem(self, p, i, j):\n",
    "        # copy the parameters into statics\n",
    "        self.b = p\n",
    "        self.k = j\n",
    "        self.k0 = i\n",
    "        if self.k <= self.k0 + 1:\n",
    "            return self.b # --DEPARTURE--\n",
    "\n",
    "        # With this line, strings of length 1 or 2 don't go through the\n",
    "        # stemming process, although no mention is made of this in the\n",
    "        # published algorithm. Remove the line to match the published\n",
    "        # algorithm.\n",
    "\n",
    "        self.step1ab()\n",
    "        self.step1c()\n",
    "        self.step2()\n",
    "        self.step3()\n",
    "        self.step4()\n",
    "        self.step5()\n",
    "        return self.b[self.k0:self.k+1]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    p = PorterStemmer()\n",
    "    if len(sys.argv) > 1:\n",
    "        for f in sys.argv[1:]:\n",
    "            for i in range (FileNum):\n",
    "                infile = (open(a[i], 'r'))\n",
    "                tmp = infile\n",
    "                output = ''\n",
    "                word = ''\n",
    "                if tmp == '':\n",
    "                    break\n",
    "                for c in tmp:\n",
    "                    if c.isalpha():\n",
    "                        word += c.lower()\n",
    "                    else:\n",
    "                        if word:\n",
    "                            output += p.stem(word, 0,len(word)-1)\n",
    "                            word = ''\n",
    "                        output += c.lower()\n",
    "                RawDocument.append(output)\n",
    "        infile.close()\n",
    "                \n",
    "# print(RawDocument[1094])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "\n",
    "TokenDocument = [] #有重複\n",
    "Token = []\n",
    "\n",
    "for i in range (FileNum): \n",
    "    TokenDocument.append(word_tokenize(RawDocument[i]))\n",
    "    \n",
    "# print(len(TokenDocument[1094]))\n",
    "\n",
    "# Stopword removal\n",
    "# Redundancy removal\n",
    "\n",
    "redundency = ['!','.','?','$',\"'s\",\"''\",',',\"n't\",\"''\",\"'ve\",'would','c']\n",
    "stop_words = set(stopwords.words('english') + redundency)#為什麼去不掉句點？\n",
    "# print(len(stop_words),len(w),stopwords.words('english'))\n",
    "for i in range (FileNum): \n",
    "    tmp = []\n",
    "    for j in range(len(TokenDocument[i])): \n",
    "        if (TokenDocument[i][j] not in stop_words) and (TokenDocument[i][j].isalpha() == True):\n",
    "            tmp.append(TokenDocument[i][j])\n",
    "    Token.append(tmp)\n",
    "#     print(Token[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort\n",
    "for i in range(FileNum):  \n",
    "    Token[i].sort()\n",
    "# print(len(Token[1094]))\n",
    "\n",
    "# Establish my classfied Documents\n",
    "\n",
    "TermDocument = [] #每個文檔去掉重複\n",
    "Dictionary = [] #13類的共用字典\n",
    "\n",
    "for i in range(FileNum): \n",
    "    tmp = []\n",
    "    for k in range(len(Token[i])):\n",
    "        if Token[i][k] not in tmp:\n",
    "            tmp.append(Token[i][k])\n",
    "    tmp.sort()\n",
    "    TermDocument.append(tmp)\n",
    "#     print(TermDocument[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "773 250 250\n",
      "1060 250 250\n",
      "1389 250 250\n",
      "1109 250 250\n",
      "1460 250 250\n",
      "843 250 250\n",
      "1316 250 250\n",
      "902 250 250\n",
      "1192 250 250\n",
      "1183 250 250\n",
      "1636 250 250\n",
      "1481 250 250\n",
      "1236 250 250\n",
      "{'china': 0.014131897711978465, 'importance': 0.0033647375504710633, 'korea': 0.006729475100942127, 'doubt': 0.006729475100942127, 'future': 0.006056527590847914, 'beginning': 0.008075370121130552, 'ceremony': 0.006729475100942127, 'daughter': 0.006729475100942127, 'talks': 0.013458950201884253, 'visit': 0.040376850605652756, 'pursue': 0.005383580080753701, 'resolve': 0.005383580080753701, 'sees': 0.0033647375504710633, 'communist': 0.014804845222072678, 'sidelines': 0.005383580080753701, 'prosperity': 0.0033647375504710633, 'generally': 0.004710632570659489, 'unaccounted': 0.006729475100942127, 'nixon': 0.005383580080753701, 'business': 0.01816958277254374, 'continues': 0.004710632570659489, 'learned': 0.004710632570659489, 'hillary': 0.005383580080753701, 'relations': 0.010767160161507403, 'rodham': 0.0033647375504710633, 'assured': 0.0033647375504710633, 'asia': 0.013458950201884253, 'action': 0.006056527590847914, 'occasionally': 0.0033647375504710633, 'quarter': 0.004037685060565276, 'kim': 0.006729475100942127, 'century': 0.004710632570659489, 'chinese': 0.004710632570659489, 'troops': 0.004037685060565276, 'mostly': 0.004710632570659489, 'investment': 0.004037685060565276, 'richard': 0.006729475100942127, 'trip': 0.02288021534320323, 'identification': 0.004710632570659489, 'beach': 0.0033647375504710633, 'mystery': 0.004037685060565276, 'brunei': 0.01547779273216689, 'visits': 0.005383580080753701, 'interest': 0.004037685060565276, 'rice': 0.007402422611036339, 'listed': 0.004037685060565276, 'trade': 0.023553162853297442, 'accompanied': 0.004710632570659489, 'agreement': 0.009421265141318977, 'apec': 0.004710632570659489, 'allowing': 0.0033647375504710633, 'rim': 0.004710632570659489, 'relationship': 0.010094212651413189, 'program': 0.0033647375504710633, 'chi': 0.008075370121130552, 'global': 0.007402422611036339, 'ho': 0.009421265141318977, 'minh': 0.008748317631224764, 'aging': 0.0013458950201884253, 'amerasian': 0.0013458950201884253, 'amounts': 0.0013458950201884253, 'bureaucracy': 0.002018842530282638, 'crossing': 0.0026917900403768506, 'echo': 0.002018842530282638, 'emigrate': 0.0026917900403768506, 'flow': 0.002018842530282638, 'formerly': 0.0033647375504710633, 'le': 0.0026917900403768506, 'lingering': 0.002018842530282638, 'nguyen': 0.0033647375504710633, 'saigon': 0.008748317631224764, 'vietnamese': 0.03633916554508748, 'agonizing': 0.002018842530282638, 'borneo': 0.0033647375504710633, 'cancel': 0.002018842530282638, 'chapter': 0.004037685060565276, 'chelsea': 0.004037685060565276, 'christopher': 0.004710632570659489, 'commerce': 0.004037685060565276, 'deny': 0.002018842530282638, 'diplomatic': 0.0033647375504710633, 'draft': 0.0026917900403768506, 'embargo': 0.0033647375504710633, 'excavation': 0.006056527590847914, 'farewell': 0.004037685060565276, 'hanoi': 0.02826379542395693, 'hawaii': 0.004710632570659489, 'jiang': 0.004037685060565276, 'korean': 0.004710632570659489, 'lifted': 0.0033647375504710633, 'marking': 0.002018842530282638, 'mia': 0.0033647375504710633, 'mori': 0.004037685060565276, 'negotiations': 0.0033647375504710633, 'paddy': 0.005383580080753701, 'protester': 0.002018842530282638, 'rapprochement': 0.002018842530282638, 'reopened': 0.002018842530282638, 'repatriation': 0.0026917900403768506, 'roughly': 0.002018842530282638, 'struggled': 0.002018842530282638, 'sultanate': 0.0033647375504710633, 'surrendered': 0.0033647375504710633, 'suspense': 0.002018842530282638, 'sweeping': 0.002018842530282638, 'unconditionally': 0.002018842530282638, 'undertaken': 0.0026917900403768506, 'unfettered': 0.0026917900403768506, 'unified': 0.006729475100942127, 'vigorous': 0.0026917900403768506, 'waiver': 0.002018842530282638, 'warren': 0.0026917900403768506, 'yoshiro': 0.004037685060565276, 'zemin': 0.004037685060565276, 'agrees': 0.0013458950201884253, 'bilateral': 0.0033647375504710633, 'dy': 0.002018842530282638, 'easier': 0.0026917900403768506, 'liberalization': 0.002018842530282638, 'markets': 0.004710632570659489, 'nien': 0.002018842530282638, 'normalizing': 0.002018842530282638, 'rancor': 0.002018842530282638, 'servicemen': 0.006729475100942127, 'symbolic': 0.0026917900403768506, 'urgency': 0.002018842530282638, 'accounted': 0.0033647375504710633, 'accounting': 0.005383580080753701, 'ann': 0.002018842530282638, 'anthropologist': 0.002018842530282638, 'anthropologists': 0.002018842530282638, 'archaeological': 0.002018842530282638, 'bones': 0.002018842530282638, 'bunch': 0.004710632570659489, 'cambodia': 0.004710632570659489, 'captive': 0.002018842530282638, 'caucasians': 0.002018842530282638, 'closure': 0.002018842530282638, 'clue': 0.002018842530282638, 'cody': 0.0026917900403768506, 'costly': 0.002018842530282638, 'crashes': 0.002018842530282638, 'crater': 0.002018842530282638, 'culmination': 0.002018842530282638, 'digs': 0.002018842530282638, 'discrepancies': 0.002018842530282638, 'evert': 0.004037685060565276, 'fighter': 0.0033647375504710633, 'fills': 0.002018842530282638, 'forensics': 0.002018842530282638, 'fullest': 0.002018842530282638, 'highlight': 0.002018842530282638, 'hoes': 0.002018842530282638, 'hollywood': 0.002018842530282638, 'indochina': 0.002018842530282638, 'jet': 0.0026917900403768506, 'laborers': 0.002018842530282638, 'laos': 0.004710632570659489, 'lawrence': 0.0026917900403768506, 'longstanding': 0.002018842530282638, 'mias': 0.002018842530282638, 'mounds': 0.002018842530282638, 'movies': 0.002018842530282638, 'oversees': 0.002018842530282638, 'painstakingly': 0.002018842530282638, 'pays': 0.002018842530282638, 'portion': 0.002018842530282638, 'pows': 0.006729475100942127, 'puzzle': 0.002018842530282638, 'reaches': 0.002018842530282638, 'reaffirmed': 0.002018842530282638, 'reiterated': 0.002018842530282638, 'remotest': 0.002018842530282638, 'repatriated': 0.002018842530282638, 'revealing': 0.002018842530282638, 'select': 0.002018842530282638, 'sieves': 0.002018842530282638, 'sift': 0.002018842530282638, 'sliver': 0.002018842530282638, 'spending': 0.0026917900403768506, 'stony': 0.002018842530282638, 'unresolved': 0.002018842530282638, 'wartime': 0.002018842530282638, 'washed': 0.002018842530282638, 'wielding': 0.002018842530282638, 'withdrawal': 0.002018842530282638, 'withheld': 0.002018842530282638, 'benefits': 0.0033647375504710633, 'commitments': 0.002018842530282638, 'confounded': 0.002018842530282638, 'conjunction': 0.002018842530282638, 'customs': 0.002018842530282638, 'divisions': 0.002018842530282638, 'duties': 0.002018842530282638, 'empire': 0.0026917900403768506, 'exist': 0.002018842530282638, 'expanded': 0.0026917900403768506, 'expensive': 0.002018842530282638, 'focusing': 0.002018842530282638, 'gene': 0.002018842530282638, 'growth': 0.004037685060565276, 'hinges': 0.002018842530282638, 'impasse': 0.002018842530282638, 'integrated': 0.002018842530282638, 'internet': 0.004037685060565276, 'joked': 0.0026917900403768506, 'lightheartedly': 0.002018842530282638, 'moratorium': 0.002018842530282638, 'opulent': 0.0026917900403768506, 'predictions': 0.002018842530282638, 'prelude': 0.002018842530282638, 'proposition': 0.0026917900403768506, 'regain': 0.002018842530282638, 'reinforcing': 0.002018842530282638, 'societies': 0.002018842530282638, 'sperling': 0.0033647375504710633, 'stressed': 0.002018842530282638, 'successor': 0.0033647375504710633, 'turnarounds': 0.002018842530282638, 'unshaken': 0.002018842530282638, 'useful': 0.002018842530282638, 'broadcast': 0.0026917900403768506, 'reform': 0.0033647375504710633, 'restrictions': 0.002018842530282638, 'society': 0.0026917900403768506, 'acceded': 0.0013458950201884253, 'admiringly': 0.0013458950201884253, 'advice': 0.0013458950201884253, 'agenda': 0.002018842530282638, 'freedoms': 0.002018842530282638, 'itinerary': 0.002018842530282638, 'mccain': 0.002018842530282638, 'accompanying': 0.0013458950201884253, 'accords': 0.0013458950201884253, 'achieved': 0.0013458950201884253, 'airline': 0.0013458950201884253, 'apology': 0.0033647375504710633, 'contain': 0.002018842530282638, 'enemy': 0.002018842530282638, 'johnson': 0.0026917900403768506, 'lyndon': 0.0026917900403768506, 'opposed': 0.002018842530282638, 'owes': 0.002018842530282638, 'spend': 0.002018842530282638, 'accomplish': 0.0013458950201884253, 'akin': 0.0013458950201884253, 'alignment': 0.0013458950201884253, 'ambivalent': 0.0013458950201884253, 'motorcade': 0.002018842530282638, 'stepped': 0.002018842530282638}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100344.47485948623, 100004.53872508658, 100005.7924036663, 100000, 100004.87929929204, 100000, 100009.77091733049, 100010.63277377971, 100028.3559837772, 100005.88123106671, 100000, 100035.2032849328, 100034.21807941706]\n"
     ]
    }
   ],
   "source": [
    "## Naive Bayes using the Motinomial Model \n",
    "\n",
    "DocScore = [] # 1095*13\n",
    "\n",
    "ClassifiedTokenFrequncy = []\n",
    "ClassifiedTokenProb = []\n",
    "ClassSumFrequncy = []\n",
    "\n",
    "for c in range(13):\n",
    "    tmp = {}\n",
    "    tmpCnt = 0\n",
    "    for i in range(13): # 算每個classifiedToken在字典出現機率\n",
    "        for j in range(15): \n",
    "            cnt = 0\n",
    "            for k in range(len(TokenClassified[i][j])):\n",
    "                if TokenClassified[i][j][k] in FeatureSelected[c]:\n",
    "                    if TokenClassified[i][j][k] in tmp:\n",
    "                        tmp[TokenClassified[i][j][k]] = tmp[TokenClassified[i][j][k]]+1\n",
    "                        tmpCnt = tmpCnt +1\n",
    "                    else:\n",
    "                        tmp[TokenClassified[i][j][k]] = 1\n",
    "                        tmpCnt = tmpCnt+1\n",
    "    ClassifiedTokenFrequncy.append(tmp)\n",
    "    ClassSumFrequncy.append(tmpCnt)\n",
    "    \n",
    "for c in range(13):\n",
    "    print(ClassSumFrequncy[c], len(ClassifiedTokenFrequncy[c]), len(FeatureSelected[c]))\n",
    "    \n",
    "for c in range(13):\n",
    "    ClassifiedTokenProb.append(ClassifiedTokenFrequncy[c])\n",
    "    for j in range(15): \n",
    "        for k in range(len(TokenClassified[c][j])):\n",
    "            if TokenClassified[c][j][k] in ClassifiedTokenProb[c]:\n",
    "                if ClassifiedTokenProb[c].get(TokenClassified[c][j][k], 'not exist') >= 1:\n",
    "                    ClassifiedTokenProb[c][TokenClassified[c][j][k]] = (ClassifiedTokenProb[c][TokenClassified[c][j][k]]+1)/(ClassSumFrequncy[c]+len(FeatureSelected[c]))\n",
    "print(ClassifiedTokenProb[12])\n",
    "\n",
    "\n",
    "\n",
    "for i in range(FileNum):\n",
    "    tmp = []\n",
    "    for c in range(13):\n",
    "        probsum = -100000\n",
    "        for j in range(len(Token[i])):\n",
    "             if Token[i][j] in FeatureSelected[c]:\n",
    "                probsum = probsum - math.log(ClassifiedTokenProb[c].get(Token[i][j]))   \n",
    "        tmp.append(probsum)\n",
    "    DocScore.append(tmp)\n",
    "\n",
    "print(DocScore[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 5\n",
      "18 5\n",
      "20 1\n",
      "21 1\n",
      "22 1\n",
      "23 5\n",
      "24 1\n",
      "25 1\n",
      "26 1\n",
      "27 1\n",
      "28 1\n",
      "30 1\n",
      "32 1\n",
      "33 1\n",
      "34 1\n",
      "35 5\n",
      "36 1\n",
      "37 4\n",
      "38 1\n",
      "39 1\n",
      "40 8\n",
      "41 3\n",
      "42 6\n",
      "43 1\n",
      "45 1\n",
      "46 1\n",
      "47 1\n",
      "48 5\n",
      "49 5\n",
      "50 13\n",
      "51 5\n",
      "52 3\n",
      "53 1\n",
      "54 5\n",
      "55 5\n",
      "56 5\n",
      "57 5\n",
      "58 5\n",
      "59 1\n",
      "60 3\n",
      "61 4\n",
      "62 1\n",
      "63 3\n",
      "64 4\n",
      "65 1\n",
      "66 5\n",
      "67 3\n",
      "68 4\n",
      "69 11\n",
      "71 5\n",
      "72 4\n",
      "73 5\n",
      "74 5\n",
      "75 1\n",
      "76 5\n",
      "77 1\n",
      "78 5\n",
      "79 1\n",
      "80 9\n",
      "81 3\n",
      "82 4\n",
      "84 1\n",
      "85 1\n",
      "87 8\n",
      "88 4\n",
      "89 4\n",
      "90 1\n",
      "91 5\n",
      "93 7\n",
      "94 5\n",
      "95 1\n",
      "96 5\n",
      "97 1\n",
      "98 5\n",
      "99 1\n",
      "101 5\n",
      "103 1\n",
      "104 5\n",
      "105 1\n",
      "106 5\n",
      "107 1\n",
      "108 7\n",
      "109 1\n",
      "110 10\n",
      "111 9\n",
      "112 1\n",
      "114 13\n",
      "116 3\n",
      "117 9\n",
      "118 5\n",
      "119 1\n",
      "120 1\n",
      "121 1\n",
      "122 1\n",
      "123 9\n",
      "124 5\n",
      "125 3\n",
      "126 11\n",
      "127 7\n",
      "128 1\n",
      "129 5\n",
      "144 5\n",
      "146 1\n",
      "147 4\n",
      "148 2\n",
      "149 1\n",
      "150 10\n",
      "151 12\n",
      "152 2\n",
      "153 8\n",
      "154 6\n",
      "155 6\n",
      "156 6\n",
      "157 1\n",
      "158 6\n",
      "159 10\n",
      "160 6\n",
      "161 5\n",
      "162 7\n",
      "163 10\n",
      "164 2\n",
      "165 1\n",
      "166 8\n",
      "167 1\n",
      "168 11\n",
      "170 5\n",
      "171 3\n",
      "172 2\n",
      "173 5\n",
      "174 11\n",
      "175 1\n",
      "176 4\n",
      "177 4\n",
      "178 1\n",
      "179 1\n",
      "180 4\n",
      "181 4\n",
      "182 2\n",
      "183 1\n",
      "184 11\n",
      "185 10\n",
      "186 6\n",
      "187 2\n",
      "188 5\n",
      "189 13\n",
      "190 5\n",
      "191 13\n",
      "192 13\n",
      "193 1\n",
      "194 5\n",
      "195 12\n",
      "196 11\n",
      "197 5\n",
      "198 5\n",
      "199 6\n",
      "200 5\n",
      "201 2\n",
      "202 5\n",
      "203 5\n",
      "204 1\n",
      "205 1\n",
      "206 2\n",
      "207 5\n",
      "208 4\n",
      "209 4\n",
      "210 5\n",
      "211 1\n",
      "212 1\n",
      "213 1\n",
      "214 2\n",
      "215 2\n",
      "216 2\n",
      "217 3\n",
      "218 12\n",
      "219 5\n",
      "220 2\n",
      "221 2\n",
      "222 1\n",
      "223 5\n",
      "224 2\n",
      "225 10\n",
      "226 5\n",
      "227 5\n",
      "228 5\n",
      "229 12\n",
      "230 12\n",
      "231 5\n",
      "232 7\n",
      "233 1\n",
      "234 2\n",
      "235 5\n",
      "236 2\n",
      "237 2\n",
      "238 1\n",
      "239 1\n",
      "242 6\n",
      "246 3\n",
      "247 4\n",
      "249 5\n",
      "251 2\n",
      "252 4\n",
      "253 5\n",
      "257 5\n",
      "259 3\n",
      "261 2\n",
      "263 5\n",
      "264 2\n",
      "265 5\n",
      "266 4\n",
      "267 1\n",
      "268 3\n",
      "269 3\n",
      "270 2\n",
      "271 5\n",
      "272 2\n",
      "273 13\n",
      "274 2\n",
      "276 4\n",
      "277 5\n",
      "280 1\n",
      "281 4\n",
      "282 4\n",
      "283 4\n",
      "284 5\n",
      "285 11\n",
      "286 2\n",
      "287 2\n",
      "288 2\n",
      "289 5\n",
      "290 2\n",
      "291 2\n",
      "292 5\n",
      "293 5\n",
      "294 1\n",
      "297 1\n",
      "298 2\n",
      "299 1\n",
      "300 1\n",
      "302 2\n",
      "303 4\n",
      "306 1\n",
      "307 2\n",
      "310 8\n",
      "311 5\n",
      "312 11\n",
      "313 2\n",
      "314 2\n",
      "318 10\n",
      "319 1\n",
      "322 1\n",
      "323 5\n",
      "329 5\n",
      "330 5\n",
      "331 9\n",
      "332 2\n",
      "333 5\n",
      "334 1\n",
      "335 2\n",
      "336 1\n",
      "339 1\n",
      "340 1\n",
      "342 2\n",
      "343 1\n",
      "344 1\n",
      "345 4\n",
      "346 1\n",
      "347 3\n",
      "348 2\n",
      "349 1\n",
      "350 1\n",
      "351 1\n",
      "352 4\n",
      "353 1\n",
      "354 2\n",
      "355 1\n",
      "356 3\n",
      "357 4\n",
      "358 1\n",
      "359 4\n",
      "360 1\n",
      "361 6\n",
      "362 1\n",
      "363 6\n",
      "364 4\n",
      "365 1\n",
      "366 2\n",
      "367 2\n",
      "368 1\n",
      "369 2\n",
      "370 3\n",
      "371 1\n",
      "372 2\n",
      "373 1\n",
      "374 2\n",
      "375 8\n",
      "376 1\n",
      "377 2\n",
      "378 1\n",
      "379 1\n",
      "380 1\n",
      "381 2\n",
      "382 1\n",
      "383 2\n",
      "384 2\n",
      "385 2\n",
      "386 2\n",
      "387 1\n",
      "388 2\n",
      "389 4\n",
      "390 1\n",
      "391 1\n",
      "392 8\n",
      "393 8\n",
      "394 1\n",
      "395 1\n",
      "396 2\n",
      "398 1\n",
      "399 1\n",
      "400 2\n",
      "402 2\n",
      "403 1\n",
      "404 7\n",
      "405 4\n",
      "406 1\n",
      "407 1\n",
      "408 1\n",
      "409 6\n",
      "410 1\n",
      "411 1\n",
      "412 1\n",
      "413 4\n",
      "414 2\n",
      "415 4\n",
      "416 12\n",
      "417 1\n",
      "418 2\n",
      "419 2\n",
      "420 1\n",
      "421 5\n",
      "422 11\n",
      "423 2\n",
      "424 1\n",
      "425 2\n",
      "426 12\n",
      "427 1\n",
      "428 2\n",
      "429 5\n",
      "430 5\n",
      "431 2\n",
      "432 2\n",
      "433 5\n",
      "434 3\n",
      "435 2\n",
      "436 1\n",
      "437 6\n",
      "438 3\n",
      "439 1\n",
      "440 1\n",
      "441 6\n",
      "442 1\n",
      "444 2\n",
      "446 4\n",
      "447 2\n",
      "448 2\n",
      "449 2\n",
      "451 2\n",
      "452 2\n",
      "453 5\n",
      "454 4\n",
      "455 10\n",
      "456 10\n",
      "457 6\n",
      "458 2\n",
      "459 2\n",
      "460 2\n",
      "461 1\n",
      "462 5\n",
      "463 1\n",
      "464 8\n",
      "465 2\n",
      "467 6\n",
      "468 2\n",
      "469 2\n",
      "470 1\n",
      "471 2\n",
      "472 1\n",
      "473 4\n",
      "474 3\n",
      "475 1\n",
      "476 2\n",
      "477 2\n",
      "478 5\n",
      "479 2\n",
      "481 3\n",
      "482 2\n",
      "483 9\n",
      "484 3\n",
      "486 1\n",
      "487 1\n",
      "488 1\n",
      "489 2\n",
      "490 9\n",
      "491 1\n",
      "492 9\n",
      "493 1\n",
      "494 2\n",
      "495 1\n",
      "496 7\n",
      "497 5\n",
      "498 1\n",
      "499 1\n",
      "500 1\n",
      "501 1\n",
      "502 9\n",
      "503 9\n",
      "504 9\n",
      "505 7\n",
      "506 7\n",
      "507 9\n",
      "508 9\n",
      "509 1\n",
      "510 2\n",
      "511 6\n",
      "512 9\n",
      "514 1\n",
      "515 8\n",
      "516 9\n",
      "517 1\n",
      "518 2\n",
      "519 4\n",
      "521 1\n",
      "522 5\n",
      "524 6\n",
      "525 5\n",
      "528 1\n",
      "543 11\n",
      "544 1\n",
      "545 1\n",
      "546 10\n",
      "547 10\n",
      "548 3\n",
      "549 1\n",
      "550 3\n",
      "551 1\n",
      "552 2\n",
      "553 7\n",
      "554 4\n",
      "555 1\n",
      "556 1\n",
      "557 1\n",
      "558 1\n",
      "559 11\n",
      "560 8\n",
      "561 2\n",
      "562 8\n",
      "563 4\n",
      "564 1\n",
      "565 1\n",
      "566 4\n",
      "567 9\n",
      "568 2\n",
      "569 1\n",
      "570 6\n",
      "572 8\n",
      "577 12\n",
      "579 1\n",
      "580 3\n",
      "587 1\n",
      "589 1\n",
      "590 2\n",
      "591 9\n",
      "592 3\n",
      "593 4\n",
      "594 5\n",
      "595 1\n",
      "596 1\n",
      "597 1\n",
      "598 6\n",
      "599 1\n",
      "600 1\n",
      "601 1\n",
      "602 6\n",
      "603 1\n",
      "604 4\n",
      "605 1\n",
      "606 1\n",
      "607 1\n",
      "608 4\n",
      "609 3\n",
      "610 3\n",
      "611 10\n",
      "612 2\n",
      "613 3\n",
      "614 3\n",
      "615 6\n",
      "616 9\n",
      "617 4\n",
      "618 1\n",
      "619 11\n",
      "620 2\n",
      "621 1\n",
      "622 1\n",
      "623 1\n",
      "624 1\n",
      "625 4\n",
      "626 5\n",
      "627 1\n",
      "628 1\n",
      "629 3\n",
      "630 3\n",
      "631 1\n",
      "632 6\n",
      "633 7\n",
      "634 1\n",
      "636 8\n",
      "637 5\n",
      "638 1\n",
      "639 5\n",
      "640 5\n",
      "641 1\n",
      "642 1\n",
      "643 3\n",
      "644 3\n",
      "645 1\n",
      "647 2\n",
      "648 5\n",
      "649 7\n",
      "650 1\n",
      "651 1\n",
      "652 5\n",
      "653 2\n",
      "654 1\n",
      "655 1\n",
      "656 1\n",
      "657 1\n",
      "658 11\n",
      "659 2\n",
      "660 5\n",
      "661 2\n",
      "662 5\n",
      "663 5\n",
      "664 1\n",
      "665 1\n",
      "666 1\n",
      "667 1\n",
      "668 1\n",
      "669 11\n",
      "670 1\n",
      "671 1\n",
      "672 1\n",
      "673 2\n",
      "674 1\n",
      "675 1\n",
      "676 2\n",
      "677 2\n",
      "678 3\n",
      "679 5\n",
      "681 2\n",
      "682 1\n",
      "684 2\n",
      "685 1\n",
      "686 1\n",
      "687 5\n",
      "688 1\n",
      "689 1\n",
      "690 1\n",
      "691 2\n",
      "692 2\n",
      "693 2\n",
      "694 3\n",
      "695 6\n",
      "696 1\n",
      "697 10\n",
      "698 1\n",
      "699 6\n",
      "701 7\n",
      "703 1\n",
      "707 5\n",
      "710 1\n",
      "711 12\n",
      "712 1\n",
      "713 1\n",
      "714 3\n",
      "715 6\n",
      "716 1\n",
      "717 1\n",
      "718 1\n",
      "721 6\n",
      "725 3\n",
      "727 1\n",
      "728 2\n",
      "729 4\n",
      "734 5\n",
      "736 2\n",
      "737 2\n",
      "738 11\n",
      "739 1\n",
      "741 5\n",
      "742 2\n",
      "743 6\n",
      "745 2\n",
      "746 1\n",
      "747 2\n",
      "748 1\n",
      "749 7\n",
      "750 1\n",
      "753 5\n",
      "758 1\n",
      "761 1\n",
      "762 5\n",
      "763 1\n",
      "764 3\n",
      "765 2\n",
      "766 1\n",
      "767 5\n",
      "768 5\n",
      "769 2\n",
      "770 1\n",
      "771 1\n",
      "772 2\n",
      "773 7\n",
      "774 1\n",
      "775 2\n",
      "776 2\n",
      "777 2\n",
      "778 6\n",
      "779 5\n",
      "780 3\n",
      "782 3\n",
      "783 11\n",
      "784 1\n",
      "785 6\n",
      "786 2\n",
      "787 11\n",
      "788 1\n",
      "789 2\n",
      "790 2\n",
      "791 5\n",
      "792 3\n",
      "793 2\n",
      "795 3\n",
      "796 3\n",
      "797 5\n",
      "800 2\n",
      "802 2\n",
      "803 1\n",
      "804 11\n",
      "805 1\n",
      "806 1\n",
      "807 1\n",
      "808 6\n",
      "809 3\n",
      "810 2\n",
      "811 5\n",
      "814 1\n",
      "816 6\n",
      "827 3\n",
      "834 2\n",
      "835 1\n",
      "836 1\n",
      "837 1\n",
      "838 1\n",
      "843 4\n",
      "844 1\n",
      "845 1\n",
      "846 2\n",
      "847 4\n",
      "848 8\n",
      "849 8\n",
      "850 1\n",
      "851 1\n",
      "852 2\n",
      "853 2\n",
      "854 1\n",
      "855 2\n",
      "856 2\n",
      "857 5\n",
      "858 2\n",
      "859 1\n",
      "860 1\n",
      "861 1\n",
      "862 2\n",
      "863 11\n",
      "864 5\n",
      "865 5\n",
      "866 11\n",
      "867 1\n",
      "868 13\n",
      "869 13\n",
      "870 1\n",
      "871 1\n",
      "872 1\n",
      "873 13\n",
      "874 2\n",
      "875 1\n",
      "876 5\n",
      "877 1\n",
      "878 1\n",
      "879 4\n",
      "880 2\n",
      "881 2\n",
      "882 1\n",
      "883 3\n",
      "884 1\n",
      "885 1\n",
      "886 1\n",
      "887 1\n",
      "888 1\n",
      "889 2\n",
      "890 1\n",
      "891 1\n",
      "892 4\n",
      "893 8\n",
      "894 1\n",
      "895 1\n",
      "896 1\n",
      "897 1\n",
      "898 1\n",
      "899 5\n",
      "900 6\n",
      "901 8\n",
      "902 2\n",
      "903 1\n",
      "904 1\n",
      "905 1\n",
      "906 5\n",
      "907 9\n",
      "908 1\n",
      "909 1\n",
      "910 1\n",
      "911 1\n",
      "912 1\n",
      "913 1\n",
      "914 2\n",
      "915 1\n",
      "916 5\n",
      "917 5\n",
      "918 1\n",
      "919 1\n",
      "920 2\n",
      "921 1\n",
      "922 1\n",
      "923 1\n",
      "924 1\n",
      "925 1\n",
      "926 5\n",
      "927 2\n",
      "928 5\n",
      "929 9\n",
      "930 1\n",
      "931 1\n",
      "932 1\n",
      "933 5\n",
      "934 1\n",
      "935 8\n",
      "936 5\n",
      "937 1\n",
      "938 5\n",
      "939 4\n",
      "940 6\n",
      "941 1\n",
      "942 1\n",
      "943 8\n",
      "944 8\n",
      "945 1\n",
      "946 9\n",
      "947 1\n",
      "948 2\n",
      "949 1\n",
      "950 1\n",
      "951 2\n",
      "952 1\n",
      "953 1\n",
      "954 1\n",
      "955 1\n",
      "956 1\n",
      "957 8\n",
      "958 1\n",
      "959 1\n",
      "960 1\n",
      "961 1\n",
      "962 1\n",
      "963 1\n",
      "964 10\n",
      "965 1\n",
      "966 6\n",
      "967 2\n",
      "968 1\n",
      "969 1\n",
      "970 1\n",
      "971 1\n",
      "972 1\n",
      "973 1\n",
      "974 1\n",
      "975 2\n",
      "976 2\n",
      "977 1\n",
      "978 1\n",
      "979 1\n",
      "980 2\n",
      "981 1\n",
      "982 2\n",
      "983 2\n",
      "984 3\n",
      "985 10\n",
      "986 5\n",
      "987 13\n",
      "988 1\n",
      "989 5\n",
      "990 2\n",
      "991 1\n",
      "992 2\n",
      "993 1\n",
      "994 1\n",
      "996 1\n",
      "997 1\n",
      "1000 1\n",
      "1001 1\n",
      "1002 1\n",
      "1004 1\n",
      "1008 1\n",
      "1010 1\n",
      "1017 1\n",
      "1018 1\n",
      "1020 10\n",
      "1021 7\n",
      "1022 10\n",
      "1023 5\n",
      "1024 1\n",
      "1025 5\n",
      "1026 4\n",
      "1027 10\n",
      "1028 2\n",
      "1029 1\n",
      "1030 2\n",
      "1031 5\n",
      "1032 2\n",
      "1033 1\n",
      "1034 5\n",
      "1035 1\n",
      "1036 5\n",
      "1037 1\n",
      "1038 1\n",
      "1039 5\n",
      "1040 5\n",
      "1041 5\n",
      "1042 5\n",
      "1043 1\n",
      "1044 1\n",
      "1045 4\n",
      "1046 2\n",
      "1047 1\n",
      "1048 2\n",
      "1049 2\n",
      "1050 1\n",
      "1051 11\n",
      "1052 5\n",
      "1053 1\n",
      "1054 5\n",
      "1055 2\n",
      "1056 5\n",
      "1057 2\n",
      "1058 1\n",
      "1059 8\n",
      "1060 4\n",
      "1061 1\n",
      "1062 1\n",
      "1063 2\n",
      "1064 3\n",
      "1065 2\n",
      "1066 11\n",
      "1067 11\n",
      "1068 1\n",
      "1069 1\n",
      "1070 1\n",
      "1071 1\n",
      "1072 5\n",
      "1073 6\n",
      "1074 3\n",
      "1075 1\n",
      "1076 8\n",
      "1077 8\n",
      "1078 4\n",
      "1079 3\n",
      "1080 5\n",
      "1081 12\n",
      "1082 4\n",
      "1083 13\n",
      "1084 10\n",
      "1085 10\n",
      "1086 9\n",
      "1087 4\n",
      "1088 10\n",
      "1089 1\n",
      "1090 2\n",
      "1091 2\n",
      "1092 5\n",
      "1093 5\n",
      "1094 2\n",
      "1095 3\n"
     ]
    }
   ],
   "source": [
    "# Classify the documents\n",
    "\n",
    "SuitedClass = []\n",
    "MaxScore = 0.0\n",
    "\n",
    "for i in range(FileNum):\n",
    "#     print(DocScore[i])\n",
    "    MaxScore = float('inf')\n",
    "    Suited = 0\n",
    "    for j in range(13):\n",
    "        if DocScore[i][j] < MaxScore:\n",
    "            MaxScore = DocScore[i][j]\n",
    "            Suited = j+1\n",
    "    SuitedClass.append(Suited)\n",
    "\n",
    "tmp33 = []\n",
    "a= 0\n",
    "for i in range(13):\n",
    "    tmp33 = tmp33 + training[i]\n",
    "# print(tmp33)\n",
    "for i in range(FileNum):\n",
    "    if str(i+1) not in tmp33:\n",
    "        a = a+1\n",
    "        print(i+1,SuitedClass[i])\n",
    "# print('a',a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['11', '19', '29', '113', '115', '169', '278', '301', '316', '317', '321', '324', '325', '338', '341'], ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '12', '13', '14', '15', '16'], ['813', '817', '818', '819', '820', '821', '822', '824', '825', '826', '828', '829', '830', '832', '833'], ['635', '680', '683', '702', '704', '705', '706', '708', '709', '719', '720', '722', '723', '724', '726'], ['646', '751', '781', '794', '798', '799', '801', '812', '815', '823', '831', '839', '840', '841', '842'], ['995', '998', '999', '1003', '1005', '1006', '1007', '1009', '1011', '1012', '1013', '1014', '1015', '1016', '1019'], ['700', '730', '731', '732', '733', '735', '740', '744', '752', '754', '755', '756', '757', '759', '760'], ['262', '296', '304', '308', '337', '397', '401', '443', '445', '450', '466', '480', '513', '533', '534'], ['130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '145'], ['31', '44', '70', '83', '86', '92', '100', '102', '305', '309', '315', '320', '326', '327', '328'], ['240', '241', '243', '244', '245', '248', '250', '254', '255', '256', '258', '260', '275', '279', '295'], ['535', '542', '571', '573', '574', '575', '576', '578', '581', '582', '583', '584', '585', '586', '588'], ['485', '520', '523', '526', '527', '529', '530', '531', '532', '536', '537', '538', '539', '540', '541']]\n"
     ]
    }
   ],
   "source": [
    "training = []\n",
    "infile = (open('13classes.txt', 'r'))\n",
    "for i in range(13):\n",
    "    training.append(word_tokenize((infile.readline())))\n",
    "    training[i].remove(training[i][0])\n",
    "infile.close()\n",
    "\n",
    "print(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 2 Doc 1 failll\n",
      "class 2 Doc 2 failll\n",
      "class 2 Doc 3 failll\n",
      "class 2 Doc 4 failll\n",
      "class 2 Doc 5 failll\n",
      "class 2 Doc 6 failll\n",
      "class 2 Doc 7 failll\n",
      "class 2 Doc 8 failll\n",
      "class 2 Doc 9 failll\n",
      "class 2 Doc 10 failll\n",
      "class 1 Doc 11 bingo!!\n",
      "class 2 Doc 12 failll\n",
      "class 2 Doc 13 failll\n",
      "class 2 Doc 14 failll\n",
      "class 2 Doc 15 failll\n",
      "class 2 Doc 16 failll\n",
      "class 1 Doc 19 bingo!!\n",
      "class 1 Doc 29 bingo!!\n",
      "class 10 Doc 31 failll\n",
      "class 10 Doc 44 failll\n",
      "class 10 Doc 70 failll\n",
      "class 10 Doc 83 failll\n",
      "class 10 Doc 86 failll\n",
      "class 10 Doc 92 failll\n",
      "class 10 Doc 100 failll\n",
      "class 10 Doc 102 failll\n",
      "class 1 Doc 113 failll\n",
      "class 1 Doc 115 failll\n",
      "class 9 Doc 130 failll\n",
      "class 9 Doc 131 failll\n",
      "class 9 Doc 132 failll\n",
      "class 9 Doc 133 failll\n",
      "class 9 Doc 134 failll\n",
      "class 9 Doc 135 failll\n",
      "class 9 Doc 136 failll\n",
      "class 9 Doc 137 failll\n",
      "class 9 Doc 138 failll\n",
      "class 9 Doc 139 failll\n",
      "class 9 Doc 140 failll\n",
      "class 9 Doc 141 failll\n",
      "class 9 Doc 142 failll\n",
      "class 9 Doc 143 failll\n",
      "class 9 Doc 145 failll\n",
      "class 1 Doc 169 failll\n",
      "class 11 Doc 240 failll\n",
      "class 11 Doc 241 failll\n",
      "class 11 Doc 243 failll\n",
      "class 11 Doc 244 failll\n",
      "class 11 Doc 245 failll\n",
      "class 11 Doc 248 failll\n",
      "class 11 Doc 250 failll\n",
      "class 11 Doc 254 failll\n",
      "class 11 Doc 255 failll\n",
      "class 11 Doc 256 failll\n",
      "class 11 Doc 258 failll\n",
      "class 11 Doc 260 failll\n",
      "class 8 Doc 262 failll\n",
      "class 11 Doc 275 failll\n",
      "class 1 Doc 278 failll\n",
      "class 11 Doc 279 failll\n",
      "class 11 Doc 295 failll\n",
      "class 8 Doc 296 failll\n",
      "class 1 Doc 301 failll\n",
      "class 8 Doc 304 failll\n",
      "class 10 Doc 305 failll\n",
      "class 8 Doc 308 failll\n",
      "class 10 Doc 309 failll\n",
      "class 10 Doc 315 failll\n",
      "class 1 Doc 316 failll\n",
      "class 1 Doc 317 failll\n",
      "class 10 Doc 320 failll\n",
      "class 1 Doc 321 bingo!!\n",
      "class 1 Doc 324 failll\n",
      "class 1 Doc 325 bingo!!\n",
      "class 10 Doc 326 failll\n",
      "class 10 Doc 327 failll\n",
      "class 10 Doc 328 failll\n",
      "class 8 Doc 337 failll\n",
      "class 1 Doc 338 bingo!!\n",
      "class 1 Doc 341 failll\n",
      "class 8 Doc 397 failll\n",
      "class 8 Doc 401 failll\n",
      "class 8 Doc 443 failll\n",
      "class 8 Doc 445 failll\n",
      "class 8 Doc 450 failll\n",
      "class 8 Doc 466 failll\n",
      "class 8 Doc 480 failll\n",
      "class 13 Doc 485 failll\n",
      "class 8 Doc 513 failll\n",
      "class 13 Doc 520 failll\n",
      "class 13 Doc 523 failll\n",
      "class 13 Doc 526 failll\n",
      "class 13 Doc 527 failll\n",
      "class 13 Doc 529 failll\n",
      "class 13 Doc 530 failll\n",
      "class 13 Doc 531 failll\n",
      "class 13 Doc 532 failll\n",
      "class 8 Doc 533 failll\n",
      "class 8 Doc 534 failll\n",
      "class 12 Doc 535 failll\n",
      "class 13 Doc 536 failll\n",
      "class 13 Doc 537 failll\n",
      "class 13 Doc 538 failll\n",
      "class 13 Doc 539 failll\n",
      "class 13 Doc 540 failll\n",
      "class 13 Doc 541 failll\n",
      "class 12 Doc 542 failll\n",
      "class 12 Doc 571 failll\n",
      "class 12 Doc 573 failll\n",
      "class 12 Doc 574 failll\n",
      "class 12 Doc 575 failll\n",
      "class 12 Doc 576 bingo!!\n",
      "class 12 Doc 578 failll\n",
      "class 12 Doc 581 failll\n",
      "class 12 Doc 582 failll\n",
      "class 12 Doc 583 failll\n",
      "class 12 Doc 584 failll\n",
      "class 12 Doc 585 failll\n",
      "class 12 Doc 586 failll\n",
      "class 12 Doc 588 failll\n",
      "class 4 Doc 635 failll\n",
      "class 5 Doc 646 failll\n",
      "class 4 Doc 680 failll\n",
      "class 4 Doc 683 failll\n",
      "class 7 Doc 700 bingo!!\n",
      "class 4 Doc 702 failll\n",
      "class 4 Doc 704 failll\n",
      "class 4 Doc 705 failll\n",
      "class 4 Doc 706 failll\n",
      "class 4 Doc 708 failll\n",
      "class 4 Doc 709 failll\n",
      "class 4 Doc 719 failll\n",
      "class 4 Doc 720 failll\n",
      "class 4 Doc 722 failll\n",
      "class 4 Doc 723 failll\n",
      "class 4 Doc 724 failll\n",
      "class 4 Doc 726 failll\n",
      "class 7 Doc 730 failll\n",
      "class 7 Doc 731 failll\n",
      "class 7 Doc 732 failll\n",
      "class 7 Doc 733 failll\n",
      "class 7 Doc 735 failll\n",
      "class 7 Doc 740 failll\n",
      "class 7 Doc 744 failll\n",
      "class 5 Doc 751 failll\n",
      "class 7 Doc 752 failll\n",
      "class 7 Doc 754 failll\n",
      "class 7 Doc 755 failll\n",
      "class 7 Doc 756 failll\n",
      "class 7 Doc 757 failll\n",
      "class 7 Doc 759 failll\n",
      "class 7 Doc 760 failll\n",
      "class 5 Doc 781 failll\n",
      "class 5 Doc 794 failll\n",
      "class 5 Doc 798 failll\n",
      "class 5 Doc 799 failll\n",
      "class 5 Doc 801 failll\n",
      "class 5 Doc 812 failll\n",
      "class 3 Doc 813 failll\n",
      "class 5 Doc 815 failll\n",
      "class 3 Doc 817 failll\n",
      "class 3 Doc 818 failll\n",
      "class 3 Doc 819 failll\n",
      "class 3 Doc 820 failll\n",
      "class 3 Doc 821 failll\n",
      "class 3 Doc 822 failll\n",
      "class 5 Doc 823 failll\n",
      "class 3 Doc 824 failll\n",
      "class 3 Doc 825 failll\n",
      "class 3 Doc 826 bingo!!\n",
      "class 3 Doc 828 failll\n",
      "class 3 Doc 829 failll\n",
      "class 3 Doc 830 failll\n",
      "class 5 Doc 831 failll\n",
      "class 3 Doc 832 failll\n",
      "class 3 Doc 833 failll\n",
      "class 5 Doc 839 failll\n",
      "class 5 Doc 840 failll\n",
      "class 5 Doc 841 failll\n",
      "class 5 Doc 842 failll\n",
      "class 6 Doc 995 failll\n",
      "class 6 Doc 998 failll\n",
      "class 6 Doc 999 failll\n",
      "class 6 Doc 1003 failll\n",
      "class 6 Doc 1005 failll\n",
      "class 6 Doc 1006 failll\n",
      "class 6 Doc 1007 failll\n",
      "class 6 Doc 1009 failll\n",
      "class 6 Doc 1011 failll\n",
      "class 6 Doc 1012 failll\n",
      "class 6 Doc 1013 failll\n",
      "class 6 Doc 1014 failll\n",
      "class 6 Doc 1015 failll\n",
      "class 6 Doc 1016 failll\n",
      "class 6 Doc 1019 failll\n",
      "success 9 fail 186\n"
     ]
    }
   ],
   "source": [
    "# print(SuitedClass[19])\n",
    "\n",
    "cnt=0\n",
    "cnt1=0\n",
    "\n",
    "for i in range(1,1096):\n",
    "    for j in range(13):\n",
    "        for k in range(15):\n",
    "#             print('i',i,'training',training[j][k])\n",
    "            if i == int(training[j][k]): \n",
    "#                 print(i)\n",
    "                if j == SuitedClass[i]-1:\n",
    "                    cnt = cnt+1\n",
    "                    print('class',j+1,'Doc',i,'bingo!!')\n",
    "                else:\n",
    "                    print('class',j+1,'Doc',i,'failll')\n",
    "                    cnt1 = cnt1+1\n",
    "\n",
    "print('success',cnt,'fail',cnt1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
